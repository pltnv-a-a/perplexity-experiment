{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8dcbcb395d4e4db3b6f3a0120b3bbbb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e288850b519c4629948658495c534976",
              "IPY_MODEL_2e490250e63048ef9ce3a5598e27f3de",
              "IPY_MODEL_97d9a678fb6b4b449fced9a312c57259"
            ],
            "layout": "IPY_MODEL_772bb675077d422cb5b60e4ec2615d27"
          }
        },
        "e288850b519c4629948658495c534976": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ba78ea845164b8782fbc93fe9272341",
            "placeholder": "​",
            "style": "IPY_MODEL_c2f825018e944b258ae39e155cdc9c0e",
            "value": "Fetching 7 files: 100%"
          }
        },
        "2e490250e63048ef9ce3a5598e27f3de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8deeceae1a77486ab01df68349b4ccf1",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_abae48242ed84a2f8db93481c61a9671",
            "value": 7
          }
        },
        "97d9a678fb6b4b449fced9a312c57259": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_caac4e1e36244278be32509f908b9764",
            "placeholder": "​",
            "style": "IPY_MODEL_f87541e7e79b4bb080471591011c2c07",
            "value": " 7/7 [00:25&lt;00:00,  6.92s/it]"
          }
        },
        "772bb675077d422cb5b60e4ec2615d27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ba78ea845164b8782fbc93fe9272341": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2f825018e944b258ae39e155cdc9c0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8deeceae1a77486ab01df68349b4ccf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abae48242ed84a2f8db93481c61a9671": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "caac4e1e36244278be32509f908b9764": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f87541e7e79b4bb080471591011c2c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2598f35d31324e98bd1b17e92e201371": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_735c526a0f694040a50dd328ef5262a7",
              "IPY_MODEL_990901fe71c04a7888c3e1e440cd1c45",
              "IPY_MODEL_b3a5409245664c088b125db1d9f24f35"
            ],
            "layout": "IPY_MODEL_9e21031c4e9f49879bb4d0dcb8288834"
          }
        },
        "735c526a0f694040a50dd328ef5262a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6fb9be90c9542608612d4bf35331402",
            "placeholder": "​",
            "style": "IPY_MODEL_e3862539d4484563b70c9f7e283d4da9",
            "value": "config.json: 100%"
          }
        },
        "990901fe71c04a7888c3e1e440cd1c45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a652eb366ec409d8bc1bb3e11df5aeb",
            "max": 506,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c14923d62ce4af8937c9eda8c1ae2b0",
            "value": 506
          }
        },
        "b3a5409245664c088b125db1d9f24f35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_acfbe2a2d83740c39a689e251d6fcc6a",
            "placeholder": "​",
            "style": "IPY_MODEL_6c2e88e4ea2d43f083b20d72593889d5",
            "value": " 506/506 [00:00&lt;00:00, 16.9kB/s]"
          }
        },
        "9e21031c4e9f49879bb4d0dcb8288834": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6fb9be90c9542608612d4bf35331402": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3862539d4484563b70c9f7e283d4da9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a652eb366ec409d8bc1bb3e11df5aeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c14923d62ce4af8937c9eda8c1ae2b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "acfbe2a2d83740c39a689e251d6fcc6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c2e88e4ea2d43f083b20d72593889d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72946030402e49198efef9f49b186250": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0904e795d4f459bba021e44d29587ea",
              "IPY_MODEL_559767a9b5974915b5ccbd1fbe1fecf2",
              "IPY_MODEL_10cf71a83025440c853a433cac45bbab"
            ],
            "layout": "IPY_MODEL_7ae5598737e14ad7addd1f206c66c7c7"
          }
        },
        "f0904e795d4f459bba021e44d29587ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39b034109fb443789bca4e79f5f89c8e",
            "placeholder": "​",
            "style": "IPY_MODEL_b91020db923549d9ad1de061d4f32cea",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "559767a9b5974915b5ccbd1fbe1fecf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9204a1fb0cb344e2a3f3f38c6f51cce3",
            "max": 330,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_965b2df328874b87af705334824f0d8e",
            "value": 330
          }
        },
        "10cf71a83025440c853a433cac45bbab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a89c50ad78f4a41b4a8d1891960e056",
            "placeholder": "​",
            "style": "IPY_MODEL_bef9434751c949a2922d3025c9133966",
            "value": " 330/330 [00:00&lt;00:00, 16.4kB/s]"
          }
        },
        "7ae5598737e14ad7addd1f206c66c7c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39b034109fb443789bca4e79f5f89c8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b91020db923549d9ad1de061d4f32cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9204a1fb0cb344e2a3f3f38c6f51cce3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "965b2df328874b87af705334824f0d8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a89c50ad78f4a41b4a8d1891960e056": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bef9434751c949a2922d3025c9133966": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3db49867896940779f923152a47f24e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d0d7f0ee6f2b4eb0be64fd49c2f957c0",
              "IPY_MODEL_23473dce63a549859fc34a58735d36cf",
              "IPY_MODEL_71c642100e3b43338d8434a15cd2da87"
            ],
            "layout": "IPY_MODEL_8036a761b53947c6b4571dbc6d7bd4c4"
          }
        },
        "d0d7f0ee6f2b4eb0be64fd49c2f957c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f471a0f2ef0e4528ac29edbd59d6471f",
            "placeholder": "​",
            "style": "IPY_MODEL_e43bf818bd884ba0b85435481b1efc7c",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "23473dce63a549859fc34a58735d36cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4751db61a69448328304619a711d824a",
            "max": 593,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6518cddf061245f5949511b4324039dc",
            "value": 593
          }
        },
        "71c642100e3b43338d8434a15cd2da87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2b6eadcdd844e5bb0a38b6029b91466",
            "placeholder": "​",
            "style": "IPY_MODEL_1fa09fa9a4704f408726f65fab97fb8b",
            "value": " 593/593 [00:00&lt;00:00, 37.2kB/s]"
          }
        },
        "8036a761b53947c6b4571dbc6d7bd4c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f471a0f2ef0e4528ac29edbd59d6471f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e43bf818bd884ba0b85435481b1efc7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4751db61a69448328304619a711d824a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6518cddf061245f5949511b4324039dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2b6eadcdd844e5bb0a38b6029b91466": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fa09fa9a4704f408726f65fab97fb8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a50c7c6dade744849131d4aa84c71d48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c1db95dc09dc452d802d9de16affd4f0",
              "IPY_MODEL_020376582eb24706a29e800b721acd1c",
              "IPY_MODEL_982ccfceca954eae8956ca3119e76817"
            ],
            "layout": "IPY_MODEL_5d568a29079b418f9f4984a5815047f7"
          }
        },
        "c1db95dc09dc452d802d9de16affd4f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7f9e621f7d942178c4751716c1a6448",
            "placeholder": "​",
            "style": "IPY_MODEL_b7950b2f90b047d19915e3ea36f0603c",
            "value": "generation_config.json: 100%"
          }
        },
        "020376582eb24706a29e800b721acd1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a662f3499754405882a8eb682e057493",
            "max": 137,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c48152745b0743ebbb9afc4c287aa7ee",
            "value": 137
          }
        },
        "982ccfceca954eae8956ca3119e76817": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2797849597ee482a86198221f42b2556",
            "placeholder": "​",
            "style": "IPY_MODEL_3b5875dc9d8540da80d2ee9b74025d17",
            "value": " 137/137 [00:00&lt;00:00, 19.8kB/s]"
          }
        },
        "5d568a29079b418f9f4984a5815047f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7f9e621f7d942178c4751716c1a6448": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7950b2f90b047d19915e3ea36f0603c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a662f3499754405882a8eb682e057493": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c48152745b0743ebbb9afc4c287aa7ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2797849597ee482a86198221f42b2556": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b5875dc9d8540da80d2ee9b74025d17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a116fcc866c24ce58e8baf6e1b043306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a227d92659f841a98f4aa522a5977024",
              "IPY_MODEL_8f8729632c1d4dab9e6e15bbf8dde067",
              "IPY_MODEL_1fb40563b02d4525a1c4204be0cb30e5"
            ],
            "layout": "IPY_MODEL_5c317c80f46b4fdd924ce540db26288d"
          }
        },
        "a227d92659f841a98f4aa522a5977024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_859881970ee343cab1a72f5d4723183d",
            "placeholder": "​",
            "style": "IPY_MODEL_ce98bc31dd6e43519dc42b8233a79278",
            "value": ".gitattributes: 100%"
          }
        },
        "8f8729632c1d4dab9e6e15bbf8dde067": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fc411071a254e6698e3c3d661ccf7e9",
            "max": 1477,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eeb0934631e04eaaa4cd1febf84608ab",
            "value": 1477
          }
        },
        "1fb40563b02d4525a1c4204be0cb30e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33e38e1b6f7e429ebd1f1d642dc6e5ab",
            "placeholder": "​",
            "style": "IPY_MODEL_f10d4fdb9ae04346a02ae55893d2e41f",
            "value": " 1.48k/1.48k [00:00&lt;00:00, 218kB/s]"
          }
        },
        "5c317c80f46b4fdd924ce540db26288d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "859881970ee343cab1a72f5d4723183d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce98bc31dd6e43519dc42b8233a79278": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fc411071a254e6698e3c3d661ccf7e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeb0934631e04eaaa4cd1febf84608ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33e38e1b6f7e429ebd1f1d642dc6e5ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f10d4fdb9ae04346a02ae55893d2e41f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6853279eb03443ec87744367ce0fb426": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a6b15699cb9f493eb06c3400cd750ac6",
              "IPY_MODEL_00428e7fe5294cbcaeca52669f14d6b6",
              "IPY_MODEL_73aaa030d8b6418c9af3e68f1b7c5f20"
            ],
            "layout": "IPY_MODEL_2a10e948108f45e091784cb8d6adf825"
          }
        },
        "a6b15699cb9f493eb06c3400cd750ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdabb7364c164309b49c0f758e90f1a0",
            "placeholder": "​",
            "style": "IPY_MODEL_baf092f93f804387ad0dc0298235185d",
            "value": "tokenizer.model: 100%"
          }
        },
        "00428e7fe5294cbcaeca52669f14d6b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32db0f23b3c64515850384da0e719c45",
            "max": 534194,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6fb8b2211bcf41d3a48db61dfb34af66",
            "value": 534194
          }
        },
        "73aaa030d8b6418c9af3e68f1b7c5f20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24f23d5c7e6d47a595ef8e6fcf7d4e8d",
            "placeholder": "​",
            "style": "IPY_MODEL_52a19a292c8545b6893eaab1a3d998ed",
            "value": " 534k/534k [00:00&lt;00:00, 7.87MB/s]"
          }
        },
        "2a10e948108f45e091784cb8d6adf825": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdabb7364c164309b49c0f758e90f1a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "baf092f93f804387ad0dc0298235185d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32db0f23b3c64515850384da0e719c45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fb8b2211bcf41d3a48db61dfb34af66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "24f23d5c7e6d47a595ef8e6fcf7d4e8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52a19a292c8545b6893eaab1a3d998ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "318ffe24262d4bd8a30afad62ad2a433": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0ca4c2b92164c44a7eda84b37e65daf",
              "IPY_MODEL_fb2e992f8c5049df8da0ee31a939018d",
              "IPY_MODEL_2f7c7f76cea447928e73048f1f47ed87"
            ],
            "layout": "IPY_MODEL_acdf786923e04eb1a092f7844fa59bf3"
          }
        },
        "f0ca4c2b92164c44a7eda84b37e65daf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d56ccc801f4b4df9ba0e019bdcca6b14",
            "placeholder": "​",
            "style": "IPY_MODEL_bfc3e2f5398444d68dae6ed508abe74a",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "fb2e992f8c5049df8da0ee31a939018d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a391cc73d44948ba9ad64058961763a4",
            "max": 6853038093,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd917c6b692b488a992bd85edf2eb066",
            "value": 6853038093
          }
        },
        "2f7c7f76cea447928e73048f1f47ed87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9beff3e4a41641c5b5409daa0328b288",
            "placeholder": "​",
            "style": "IPY_MODEL_dc4e46c154f64ea2a20c66a0e37e77de",
            "value": " 6.85G/6.85G [00:25&lt;00:00, 267MB/s]"
          }
        },
        "acdf786923e04eb1a092f7844fa59bf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d56ccc801f4b4df9ba0e019bdcca6b14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfc3e2f5398444d68dae6ed508abe74a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a391cc73d44948ba9ad64058961763a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd917c6b692b488a992bd85edf2eb066": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9beff3e4a41641c5b5409daa0328b288": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc4e46c154f64ea2a20c66a0e37e77de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78a0d80e763c489d86d303a26e871a71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_030be3312deb4a37b860a4f6e52bb804",
              "IPY_MODEL_80718a265dc04bddb2db4f4ab4a4c414",
              "IPY_MODEL_b437515c8add4afba32073503c76e146"
            ],
            "layout": "IPY_MODEL_453d5c8c01974f47bcf708026156fb00"
          }
        },
        "030be3312deb4a37b860a4f6e52bb804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba1245a9fe8b4c2eb52ebad85210dad0",
            "placeholder": "​",
            "style": "IPY_MODEL_6a4bcffe115042319d4d3a6626d22985",
            "value": "README.md: 100%"
          }
        },
        "80718a265dc04bddb2db4f4ab4a4c414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57ccd948c7f647f38d6ebec246dd678f",
            "max": 10464,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_118b7ed28bb843f6b2c9d05d75e0641d",
            "value": 10464
          }
        },
        "b437515c8add4afba32073503c76e146": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d3a3ee697c54b5ca873214f8496aa02",
            "placeholder": "​",
            "style": "IPY_MODEL_d02a7460b8b2401c86d25fdcb3f2b56d",
            "value": " 10.5k/10.5k [00:00&lt;00:00, 1.24MB/s]"
          }
        },
        "453d5c8c01974f47bcf708026156fb00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba1245a9fe8b4c2eb52ebad85210dad0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a4bcffe115042319d4d3a6626d22985": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57ccd948c7f647f38d6ebec246dd678f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "118b7ed28bb843f6b2c9d05d75e0641d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3d3a3ee697c54b5ca873214f8496aa02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d02a7460b8b2401c86d25fdcb3f2b56d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e903adcc63244140a5b5cf59cdbe7d06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a2777a768f274d3b9b0db7a0c6490868",
              "IPY_MODEL_14edc277992f43cf9a2ebc7d0ec31041",
              "IPY_MODEL_3a12d19a8ce148df85a30abca3997953"
            ],
            "layout": "IPY_MODEL_50d6c945a6d94433aeeab2333855e879"
          }
        },
        "a2777a768f274d3b9b0db7a0c6490868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5925f028c6424ab49cccb03fe23aae60",
            "placeholder": "​",
            "style": "IPY_MODEL_af101b1c31894bff9dfb017985d4387a",
            "value": "test-00000-of-00001.parquet: 100%"
          }
        },
        "14edc277992f43cf9a2ebc7d0ec31041": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82061aba31084d109dd2855d9c04d3d3",
            "max": 685430,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70091798ba6c4373abb6334865b7be94",
            "value": 685430
          }
        },
        "3a12d19a8ce148df85a30abca3997953": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_309f8f45c9f34240bed9a9a3552b41f3",
            "placeholder": "​",
            "style": "IPY_MODEL_5b96b04972344bd197648dadbd253d78",
            "value": " 685k/685k [00:00&lt;00:00, 13.2MB/s]"
          }
        },
        "50d6c945a6d94433aeeab2333855e879": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5925f028c6424ab49cccb03fe23aae60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af101b1c31894bff9dfb017985d4387a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82061aba31084d109dd2855d9c04d3d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70091798ba6c4373abb6334865b7be94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "309f8f45c9f34240bed9a9a3552b41f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b96b04972344bd197648dadbd253d78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81e3b859aac04ba8b51b8d72eab668a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b139834cd28f4c61b1e894de9f37a3ec",
              "IPY_MODEL_dbf29885711945ff817b9816df7eb8cb",
              "IPY_MODEL_70ab0c751bdd4f02841bdf52b0e2997c"
            ],
            "layout": "IPY_MODEL_ab2239814419470693572f792d07bf01"
          }
        },
        "b139834cd28f4c61b1e894de9f37a3ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67c8f6a8f8ee4b0fa13472e6fc0387fa",
            "placeholder": "​",
            "style": "IPY_MODEL_bc1ba2fbcc2c407184c76d1a1b56a485",
            "value": "train-00000-of-00001.parquet: 100%"
          }
        },
        "dbf29885711945ff817b9816df7eb8cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_212e10d45f68497ea80ef9a345cb4f14",
            "max": 6068114,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_556efcc1bd424ac799fc38fa190e4232",
            "value": 6068114
          }
        },
        "70ab0c751bdd4f02841bdf52b0e2997c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12118409f9a348a4b52eb70afe76f835",
            "placeholder": "​",
            "style": "IPY_MODEL_82581248391c492a838662dcdff73ec8",
            "value": " 6.07M/6.07M [00:00&lt;00:00, 122MB/s]"
          }
        },
        "ab2239814419470693572f792d07bf01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67c8f6a8f8ee4b0fa13472e6fc0387fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc1ba2fbcc2c407184c76d1a1b56a485": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "212e10d45f68497ea80ef9a345cb4f14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "556efcc1bd424ac799fc38fa190e4232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "12118409f9a348a4b52eb70afe76f835": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82581248391c492a838662dcdff73ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20e109cc704a413b855240f48d9a0045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4f6accfb04f4c56b7092561feb0062a",
              "IPY_MODEL_d3866b110b2944f490d6fc0d1b344b46",
              "IPY_MODEL_35f9ce9622fe4b8a97500577c23bb5af"
            ],
            "layout": "IPY_MODEL_59ded39fdaae4ae399f7f19976a6452c"
          }
        },
        "f4f6accfb04f4c56b7092561feb0062a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9665fa96d13a4caf96a2c346b12371d2",
            "placeholder": "​",
            "style": "IPY_MODEL_e0f64e95831e4c9d87d87bb99f3d128f",
            "value": "validation-00000-of-00001.parquet: 100%"
          }
        },
        "d3866b110b2944f490d6fc0d1b344b46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6750783bd35b4b3eb4955a7f708198e1",
            "max": 617738,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_762dae22997646ffbef2bf73edc09bff",
            "value": 617738
          }
        },
        "35f9ce9622fe4b8a97500577c23bb5af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe428b7a2815462ca4042adcac71fc92",
            "placeholder": "​",
            "style": "IPY_MODEL_b01bc440cbc54d3abe035a6a48ba2336",
            "value": " 618k/618k [00:00&lt;00:00, 74.5MB/s]"
          }
        },
        "59ded39fdaae4ae399f7f19976a6452c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9665fa96d13a4caf96a2c346b12371d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0f64e95831e4c9d87d87bb99f3d128f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6750783bd35b4b3eb4955a7f708198e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "762dae22997646ffbef2bf73edc09bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe428b7a2815462ca4042adcac71fc92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b01bc440cbc54d3abe035a6a48ba2336": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30ce1f504694467a95a02a1f5c490398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_634b2ca82de84145940c6b325918bf8c",
              "IPY_MODEL_dc993206e79f47d389915fc603330c1a",
              "IPY_MODEL_44441b5c0def4e67bafa7f857341eec5"
            ],
            "layout": "IPY_MODEL_a64de39bf7984c83b0de1a2e0c8bcc43"
          }
        },
        "634b2ca82de84145940c6b325918bf8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96675270e7604007bd939e8b528c8b46",
            "placeholder": "​",
            "style": "IPY_MODEL_52590c1a537348dbb65bef174ec03ace",
            "value": "Generating test split: 100%"
          }
        },
        "dc993206e79f47d389915fc603330c1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ac94bfc9a504b9d8d0556618e46eb05",
            "max": 4358,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3bc1f92dab0742c09b2e18e6f5b314fb",
            "value": 4358
          }
        },
        "44441b5c0def4e67bafa7f857341eec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec8fc683c5a440d0963ec1a3ffe4cad0",
            "placeholder": "​",
            "style": "IPY_MODEL_7a66d33e350141c3b155858e07b25e9d",
            "value": " 4358/4358 [00:00&lt;00:00, 256360.74 examples/s]"
          }
        },
        "a64de39bf7984c83b0de1a2e0c8bcc43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96675270e7604007bd939e8b528c8b46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52590c1a537348dbb65bef174ec03ace": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ac94bfc9a504b9d8d0556618e46eb05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bc1f92dab0742c09b2e18e6f5b314fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec8fc683c5a440d0963ec1a3ffe4cad0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a66d33e350141c3b155858e07b25e9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "743e3da6edab4465a8d1cdd6425d1c79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de856c71db88436a83b80b6bcf14b94c",
              "IPY_MODEL_606ccc5d90b24bc4a61acd8975855ad2",
              "IPY_MODEL_793147be596f4996b36aaf20a4187747"
            ],
            "layout": "IPY_MODEL_528fe40a390a44cdaf50adcdf9c0137a"
          }
        },
        "de856c71db88436a83b80b6bcf14b94c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1998a1537804cff88dad15df1d96a19",
            "placeholder": "​",
            "style": "IPY_MODEL_37c0d3a03f5d46a99190371ec67684ed",
            "value": "Generating train split: 100%"
          }
        },
        "606ccc5d90b24bc4a61acd8975855ad2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1920d2a35c24b078b17765ffe2a857d",
            "max": 36718,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0b8db95a8fb14c9fb5dfc717ea367b89",
            "value": 36718
          }
        },
        "793147be596f4996b36aaf20a4187747": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ccabe57898c41a5b134e566feb94171",
            "placeholder": "​",
            "style": "IPY_MODEL_03a880517f7e4f17b47c6448bc8433dc",
            "value": " 36718/36718 [00:00&lt;00:00, 697057.34 examples/s]"
          }
        },
        "528fe40a390a44cdaf50adcdf9c0137a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1998a1537804cff88dad15df1d96a19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37c0d3a03f5d46a99190371ec67684ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1920d2a35c24b078b17765ffe2a857d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b8db95a8fb14c9fb5dfc717ea367b89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ccabe57898c41a5b134e566feb94171": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03a880517f7e4f17b47c6448bc8433dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d16f6a28bd6645e4b9035e7998845a93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_19147105768849cd88900195c791ac54",
              "IPY_MODEL_c1b17cbfdddb450680d6e08022697603",
              "IPY_MODEL_083da32680cb4424897ec8a299875036"
            ],
            "layout": "IPY_MODEL_49fa4765f0e7489b821a826f7ae3edd9"
          }
        },
        "19147105768849cd88900195c791ac54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_620573a527864c96be1d1694f307012a",
            "placeholder": "​",
            "style": "IPY_MODEL_6d8f515efe7845dbabb2609b0a1aabe8",
            "value": "Generating validation split: 100%"
          }
        },
        "c1b17cbfdddb450680d6e08022697603": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8223ae5c41654eff8c5eed2827ec38fb",
            "max": 3760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a4201277bd45497c9784e749c87d2b38",
            "value": 3760
          }
        },
        "083da32680cb4424897ec8a299875036": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_333fa5d1ce5b42c9908918cdaef65fbd",
            "placeholder": "​",
            "style": "IPY_MODEL_986a46a7f0a746b4938c38e4a0f516f3",
            "value": " 3760/3760 [00:00&lt;00:00, 277582.69 examples/s]"
          }
        },
        "49fa4765f0e7489b821a826f7ae3edd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "620573a527864c96be1d1694f307012a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d8f515efe7845dbabb2609b0a1aabe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8223ae5c41654eff8c5eed2827ec38fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4201277bd45497c9784e749c87d2b38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "333fa5d1ce5b42c9908918cdaef65fbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "986a46a7f0a746b4938c38e4a0f516f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eead2635042243ee8e9a3ec599271b88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f1cffa1b4db84e108c150a43162e3158",
              "IPY_MODEL_0cfd087b32e947cf88d152924e9c617c",
              "IPY_MODEL_206a2583023d43b1936945ea30fa35a2"
            ],
            "layout": "IPY_MODEL_f84a7d17b8dd4c109dcef37ecc210eeb"
          }
        },
        "f1cffa1b4db84e108c150a43162e3158": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53cf2819e47f450da74a9f999ffb866a",
            "placeholder": "​",
            "style": "IPY_MODEL_2c89fed43426429fb50d82555986d53f",
            "value": "Creating CSV from Arrow format: 100%"
          }
        },
        "0cfd087b32e947cf88d152924e9c617c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96041941d66d433ea3f496cd197cb8c1",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_082f8e8aa84f42c49b1f91f73fcd13fd",
            "value": 5
          }
        },
        "206a2583023d43b1936945ea30fa35a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23567a86036a48f9a84f2548960d367b",
            "placeholder": "​",
            "style": "IPY_MODEL_7e3e22d827e241daadf788b2897ce0df",
            "value": " 5/5 [00:00&lt;00:00,  3.35ba/s]"
          }
        },
        "f84a7d17b8dd4c109dcef37ecc210eeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53cf2819e47f450da74a9f999ffb866a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c89fed43426429fb50d82555986d53f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96041941d66d433ea3f496cd197cb8c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "082f8e8aa84f42c49b1f91f73fcd13fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23567a86036a48f9a84f2548960d367b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e3e22d827e241daadf788b2897ce0df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ## 1. Установка зависимостей и компиляция llama.cpp\n",
        "\n",
        "# Установка системных зависимостей\n",
        "!apt-get update\n",
        "!apt-get install -y build-essential python3-pip\n",
        "!pip install huggingface-hub pandas matplotlib seaborn datasets\n",
        "\n",
        "# Компиляция llama.cpp\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "%cd llama.cpp\n",
        "!mkdir -p build\n",
        "%cd build\n",
        "!cmake ..\n",
        "!cmake --build . --config Release\n",
        "%cd ../..\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKJN9ta607_i",
        "outputId": "7391fcee-bfb7-40bb-dbc2-e5df2ede2f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [75.2 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,842 kB]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,788 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,140 kB]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,695 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,243 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,542 kB]\n",
            "Fetched 20.6 MB in 2s (9,703 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "The following additional packages will be installed:\n",
            "  python3-setuptools python3-wheel\n",
            "Suggested packages:\n",
            "  python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  python3-pip python3-setuptools python3-wheel\n",
            "0 upgraded, 3 newly installed, 0 to remove and 6 not upgraded.\n",
            "Need to get 1,677 kB of archives.\n",
            "After this operation, 8,968 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-setuptools all 59.6.0-1.2ubuntu0.22.04.2 [340 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-wheel all 0.37.1-2ubuntu0.22.04.1 [32.0 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-pip all 22.0.2+dfsg-1ubuntu0.5 [1,306 kB]\n",
            "Fetched 1,677 kB in 1s (1,468 kB/s)\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "(Reading database ... 122175 files and directories currently installed.)\n",
            "Preparing to unpack .../python3-setuptools_59.6.0-1.2ubuntu0.22.04.2_all.deb ...\n",
            "Unpacking python3-setuptools (59.6.0-1.2ubuntu0.22.04.2) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../python3-wheel_0.37.1-2ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../python3-pip_22.0.2+dfsg-1ubuntu0.5_all.deb ...\n",
            "Unpacking python3-pip (22.0.2+dfsg-1ubuntu0.5) ...\n",
            "Setting up python3-setuptools (59.6.0-1.2ubuntu0.22.04.2) ...\n",
            "Setting up python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Setting up python3-pip (22.0.2+dfsg-1ubuntu0.5) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.13.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2025.1.31)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "Successfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 48933, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 48933 (delta 18), reused 8 (delta 8), pack-reused 48897 (from 4)\u001b[K\n",
            "Receiving objects: 100% (48933/48933), 102.60 MiB | 35.35 MiB/s, done.\n",
            "Resolving deltas: 100% (35284/35284), done.\n",
            "/content/llama.cpp\n",
            "/content/llama.cpp/build\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\") \n",
            "-- Looking for pthread.h\n",
            "-- Looking for pthread.h - found\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE  \n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- Including CPU backend\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP: TRUE (found version \"4.5\")  \n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")  \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
            "[  4%] Built target ggml-base\n",
            "[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
            "[ 10%] Built target ggml-cpu\n",
            "[ 11%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
            "[ 11%] Built target ggml\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
            "[ 21%] Built target llama\n",
            "[ 21%] \u001b[34m\u001b[1mGenerating build details from Git\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\") \n",
            "[ 21%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[ 21%] Built target build_info\n",
            "[ 22%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 26%] Built target common\n",
            "[ 27%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 27%] Built target test-tokenizer-0\n",
            "[ 27%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 28%] Built target test-sampling\n",
            "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 29%] Built target test-grammar-parser\n",
            "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
            "[ 30%] Built target test-grammar-integration\n",
            "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 31%] Built target test-llama-grammar\n",
            "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
            "[ 32%] Built target test-chat\n",
            "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
            "[ 34%] Built target test-json-schema-to-grammar\n",
            "[ 35%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 35%] Built target test-tokenizer-1-bpe\n",
            "[ 36%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
            "[ 36%] Built target test-tokenizer-1-spm\n",
            "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
            "[ 38%] Built target test-log\n",
            "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
            "[ 39%] Built target test-chat-template\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
            "[ 41%] Built target test-arg-parser\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
            "[ 43%] Built target test-gguf\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[ 44%] Built target test-backend-ops\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
            "[ 45%] Built target test-model-load-cancel\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
            "[ 46%] Built target test-autorelease\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
            "[ 48%] Built target test-barrier\n",
            "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 49%] Built target test-quantize-fns\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 51%] Built target test-quantize-perf\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 52%] Built target test-rope\n",
            "[ 52%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
            "[ 53%] Built target test-c\n",
            "[ 54%] \u001b[32mBuilding CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
            "[ 54%] Built target llama-batched-bench\n",
            "[ 55%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
            "[ 55%] Built target llama-batched\n",
            "[ 55%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
            "[ 56%] Built target llama-embedding\n",
            "[ 56%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
            "[ 57%] Built target llama-eval-callback\n",
            "[ 57%] \u001b[32mBuilding CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gbnf-validator\u001b[0m\n",
            "[ 57%] Built target llama-gbnf-validator\n",
            "[ 57%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
            "[ 57%] Built target sha256\n",
            "[ 58%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
            "[ 58%] Built target xxhash\n",
            "[ 59%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
            "[ 59%] Built target sha1\n",
            "[ 60%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
            "[ 60%] Built target llama-gguf-hash\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
            "[ 61%] Built target llama-gguf-split\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
            "[ 62%] Built target llama-gguf\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gritlm\u001b[0m\n",
            "[ 63%] Built target llama-gritlm\n",
            "[ 63%] \u001b[32mBuilding CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
            "[ 64%] Built target llama-imatrix\n",
            "[ 64%] \u001b[32mBuilding CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-infill\u001b[0m\n",
            "[ 65%] Built target llama-infill\n",
            "[ 66%] \u001b[32mBuilding CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[ 66%] Built target llama-bench\n",
            "[ 66%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
            "[ 66%] Built target llama-lookahead\n",
            "[ 67%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
            "[ 67%] Built target llama-lookup\n",
            "[ 68%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
            "[ 68%] Built target llama-lookup-create\n",
            "[ 69%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
            "[ 69%] Built target llama-lookup-merge\n",
            "[ 70%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
            "[ 70%] Built target llama-lookup-stats\n",
            "[ 71%] \u001b[32mBuilding CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
            "[ 71%] Built target llama-cli\n",
            "[ 71%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
            "[ 72%] Built target llama-parallel\n",
            "[ 72%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
            "[ 73%] Built target llama-passkey\n",
            "[ 73%] \u001b[32mBuilding CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
            "[ 74%] Built target llama-perplexity\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[ 75%] Built target llama-quantize\n",
            "[ 76%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
            "[ 76%] Built target llama-retrieval\n",
            "[ 76%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
            "[ 76%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
            "[ 77%] Built target llama-server\n",
            "[ 77%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
            "[ 78%] Built target llama-save-load-state\n",
            "[ 79%] \u001b[32mBuilding CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\n",
            "[ 80%] Built target llama-run\n",
            "[ 81%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
            "[ 81%] Built target llama-simple\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
            "[ 82%] Built target llama-simple-chat\n",
            "[ 83%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
            "[ 83%] Built target llama-speculative\n",
            "[ 84%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
            "[ 84%] Built target llama-speculative-simple\n",
            "[ 84%] \u001b[32mBuilding CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
            "[ 85%] Built target llama-tokenize\n",
            "[ 85%] \u001b[32mBuilding CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
            "[ 86%] Built target llama-tts\n",
            "[ 87%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
            "[ 87%] Built target llama-gen-docs\n",
            "[ 87%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
            "[ 88%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 88%] \u001b[32mBuilding CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
            "[ 89%] Built target llama-cvector-generator\n",
            "[ 89%] \u001b[32mBuilding CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
            "[ 90%] Built target llama-export-lora\n",
            "[ 90%] \u001b[32mBuilding CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize-stats\u001b[0m\n",
            "[ 90%] Built target llama-quantize-stats\n",
            "[ 90%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o\u001b[0m\n",
            "[ 91%] Built target llava\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX static library libllava_static.a\u001b[0m\n",
            "[ 92%] Built target llava_static\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libllava_shared.so\u001b[0m\n",
            "[ 92%] Built target llava_shared\n",
            "[ 92%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n",
            "[ 92%] Built target mtmd\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX static library libmtmd_static.a\u001b[0m\n",
            "[ 92%] Built target mtmd_static\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd_shared.so\u001b[0m\n",
            "[ 93%] Built target mtmd_shared\n",
            "[ 93%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
            "[ 94%] Built target llama-llava-cli\n",
            "[ 95%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
            "[ 95%] Built target llama-minicpmv-cli\n",
            "[ 96%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
            "[ 96%] Built target llama-qwen2vl-cli\n",
            "[ 97%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-gemma3-cli.dir/gemma3-cli.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
            "[ 97%] Built target llama-gemma3-cli\n",
            "[ 97%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-clip-quantize-cli\u001b[0m\n",
            "[ 98%] Built target llama-llava-clip-quantize-cli\n",
            "[ 98%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
            "[ 99%] Built target llama-vdot\n",
            "[ 99%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
            "[100%] Built target llama-q8dot\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 2. Настройка параметров эксперимента\n",
        "\n",
        "import os\n",
        "from huggingface_hub import snapshot_download\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Конфигурация эксперимента\n",
        "MODEL_SIZE = \"7B\" # Выберите нужную битность из списка: \"3B\" \"7B\" \"13B\"\n",
        "CONTEXT_LENGTHS = [\"256\"]  # Длины контекста для теста\n",
        "QUANT_TYPES = [\"q3_k\", \"q4_k\", \"q5_k\", \"q8_0\"]\n",
        "\n",
        "BASE_DIR = \"/content\"\n",
        "MODEL_NAME = f\"open_llama_{MODEL_SIZE}\"\n",
        "MODEL_PATH = os.path.join(BASE_DIR, \"models\", MODEL_NAME)\n",
        "DATA_FILE = os.path.join(BASE_DIR, \"test.txt\")\n",
        "\n",
        "# Создаем директории\n",
        "os.makedirs(MODEL_PATH, exist_ok=True)"
      ],
      "metadata": {
        "id": "2WaXTNRU091m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 3. Загрузка модели\n",
        "\n",
        "# Загрузка модели с Hugging Face Hub\n",
        "snapshot_download(\n",
        "    repo_id=f\"openlm-research/{MODEL_NAME}\",\n",
        "    local_dir=MODEL_PATH,\n",
        "    ignore_patterns=[\"*.txt\", \"*.md\", \"*.safetensors\", \"*.h5\", \"*.ot\"],\n",
        "    resume_download=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "8QV5g3kn1WGT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 820,
          "referenced_widgets": [
            "8dcbcb395d4e4db3b6f3a0120b3bbbb0",
            "e288850b519c4629948658495c534976",
            "2e490250e63048ef9ce3a5598e27f3de",
            "97d9a678fb6b4b449fced9a312c57259",
            "772bb675077d422cb5b60e4ec2615d27",
            "4ba78ea845164b8782fbc93fe9272341",
            "c2f825018e944b258ae39e155cdc9c0e",
            "8deeceae1a77486ab01df68349b4ccf1",
            "abae48242ed84a2f8db93481c61a9671",
            "caac4e1e36244278be32509f908b9764",
            "f87541e7e79b4bb080471591011c2c07",
            "2598f35d31324e98bd1b17e92e201371",
            "735c526a0f694040a50dd328ef5262a7",
            "990901fe71c04a7888c3e1e440cd1c45",
            "b3a5409245664c088b125db1d9f24f35",
            "9e21031c4e9f49879bb4d0dcb8288834",
            "a6fb9be90c9542608612d4bf35331402",
            "e3862539d4484563b70c9f7e283d4da9",
            "8a652eb366ec409d8bc1bb3e11df5aeb",
            "6c14923d62ce4af8937c9eda8c1ae2b0",
            "acfbe2a2d83740c39a689e251d6fcc6a",
            "6c2e88e4ea2d43f083b20d72593889d5",
            "72946030402e49198efef9f49b186250",
            "f0904e795d4f459bba021e44d29587ea",
            "559767a9b5974915b5ccbd1fbe1fecf2",
            "10cf71a83025440c853a433cac45bbab",
            "7ae5598737e14ad7addd1f206c66c7c7",
            "39b034109fb443789bca4e79f5f89c8e",
            "b91020db923549d9ad1de061d4f32cea",
            "9204a1fb0cb344e2a3f3f38c6f51cce3",
            "965b2df328874b87af705334824f0d8e",
            "6a89c50ad78f4a41b4a8d1891960e056",
            "bef9434751c949a2922d3025c9133966",
            "3db49867896940779f923152a47f24e9",
            "d0d7f0ee6f2b4eb0be64fd49c2f957c0",
            "23473dce63a549859fc34a58735d36cf",
            "71c642100e3b43338d8434a15cd2da87",
            "8036a761b53947c6b4571dbc6d7bd4c4",
            "f471a0f2ef0e4528ac29edbd59d6471f",
            "e43bf818bd884ba0b85435481b1efc7c",
            "4751db61a69448328304619a711d824a",
            "6518cddf061245f5949511b4324039dc",
            "b2b6eadcdd844e5bb0a38b6029b91466",
            "1fa09fa9a4704f408726f65fab97fb8b",
            "a50c7c6dade744849131d4aa84c71d48",
            "c1db95dc09dc452d802d9de16affd4f0",
            "020376582eb24706a29e800b721acd1c",
            "982ccfceca954eae8956ca3119e76817",
            "5d568a29079b418f9f4984a5815047f7",
            "e7f9e621f7d942178c4751716c1a6448",
            "b7950b2f90b047d19915e3ea36f0603c",
            "a662f3499754405882a8eb682e057493",
            "c48152745b0743ebbb9afc4c287aa7ee",
            "2797849597ee482a86198221f42b2556",
            "3b5875dc9d8540da80d2ee9b74025d17",
            "a116fcc866c24ce58e8baf6e1b043306",
            "a227d92659f841a98f4aa522a5977024",
            "8f8729632c1d4dab9e6e15bbf8dde067",
            "1fb40563b02d4525a1c4204be0cb30e5",
            "5c317c80f46b4fdd924ce540db26288d",
            "859881970ee343cab1a72f5d4723183d",
            "ce98bc31dd6e43519dc42b8233a79278",
            "8fc411071a254e6698e3c3d661ccf7e9",
            "eeb0934631e04eaaa4cd1febf84608ab",
            "33e38e1b6f7e429ebd1f1d642dc6e5ab",
            "f10d4fdb9ae04346a02ae55893d2e41f",
            "6853279eb03443ec87744367ce0fb426",
            "a6b15699cb9f493eb06c3400cd750ac6",
            "00428e7fe5294cbcaeca52669f14d6b6",
            "73aaa030d8b6418c9af3e68f1b7c5f20",
            "2a10e948108f45e091784cb8d6adf825",
            "fdabb7364c164309b49c0f758e90f1a0",
            "baf092f93f804387ad0dc0298235185d",
            "32db0f23b3c64515850384da0e719c45",
            "6fb8b2211bcf41d3a48db61dfb34af66",
            "24f23d5c7e6d47a595ef8e6fcf7d4e8d",
            "52a19a292c8545b6893eaab1a3d998ed",
            "318ffe24262d4bd8a30afad62ad2a433",
            "f0ca4c2b92164c44a7eda84b37e65daf",
            "fb2e992f8c5049df8da0ee31a939018d",
            "2f7c7f76cea447928e73048f1f47ed87",
            "acdf786923e04eb1a092f7844fa59bf3",
            "d56ccc801f4b4df9ba0e019bdcca6b14",
            "bfc3e2f5398444d68dae6ed508abe74a",
            "a391cc73d44948ba9ad64058961763a4",
            "dd917c6b692b488a992bd85edf2eb066",
            "9beff3e4a41641c5b5409daa0328b288",
            "dc4e46c154f64ea2a20c66a0e37e77de",
            "78a0d80e763c489d86d303a26e871a71",
            "030be3312deb4a37b860a4f6e52bb804",
            "80718a265dc04bddb2db4f4ab4a4c414",
            "b437515c8add4afba32073503c76e146",
            "453d5c8c01974f47bcf708026156fb00",
            "ba1245a9fe8b4c2eb52ebad85210dad0",
            "6a4bcffe115042319d4d3a6626d22985",
            "57ccd948c7f647f38d6ebec246dd678f",
            "118b7ed28bb843f6b2c9d05d75e0641d",
            "3d3a3ee697c54b5ca873214f8496aa02",
            "d02a7460b8b2401c86d25fdcb3f2b56d",
            "e903adcc63244140a5b5cf59cdbe7d06",
            "a2777a768f274d3b9b0db7a0c6490868",
            "14edc277992f43cf9a2ebc7d0ec31041",
            "3a12d19a8ce148df85a30abca3997953",
            "50d6c945a6d94433aeeab2333855e879",
            "5925f028c6424ab49cccb03fe23aae60",
            "af101b1c31894bff9dfb017985d4387a",
            "82061aba31084d109dd2855d9c04d3d3",
            "70091798ba6c4373abb6334865b7be94",
            "309f8f45c9f34240bed9a9a3552b41f3",
            "5b96b04972344bd197648dadbd253d78",
            "81e3b859aac04ba8b51b8d72eab668a3",
            "b139834cd28f4c61b1e894de9f37a3ec",
            "dbf29885711945ff817b9816df7eb8cb",
            "70ab0c751bdd4f02841bdf52b0e2997c",
            "ab2239814419470693572f792d07bf01",
            "67c8f6a8f8ee4b0fa13472e6fc0387fa",
            "bc1ba2fbcc2c407184c76d1a1b56a485",
            "212e10d45f68497ea80ef9a345cb4f14",
            "556efcc1bd424ac799fc38fa190e4232",
            "12118409f9a348a4b52eb70afe76f835",
            "82581248391c492a838662dcdff73ec8",
            "20e109cc704a413b855240f48d9a0045",
            "f4f6accfb04f4c56b7092561feb0062a",
            "d3866b110b2944f490d6fc0d1b344b46",
            "35f9ce9622fe4b8a97500577c23bb5af",
            "59ded39fdaae4ae399f7f19976a6452c",
            "9665fa96d13a4caf96a2c346b12371d2",
            "e0f64e95831e4c9d87d87bb99f3d128f",
            "6750783bd35b4b3eb4955a7f708198e1",
            "762dae22997646ffbef2bf73edc09bff",
            "fe428b7a2815462ca4042adcac71fc92",
            "b01bc440cbc54d3abe035a6a48ba2336",
            "30ce1f504694467a95a02a1f5c490398",
            "634b2ca82de84145940c6b325918bf8c",
            "dc993206e79f47d389915fc603330c1a",
            "44441b5c0def4e67bafa7f857341eec5",
            "a64de39bf7984c83b0de1a2e0c8bcc43",
            "96675270e7604007bd939e8b528c8b46",
            "52590c1a537348dbb65bef174ec03ace",
            "4ac94bfc9a504b9d8d0556618e46eb05",
            "3bc1f92dab0742c09b2e18e6f5b314fb",
            "ec8fc683c5a440d0963ec1a3ffe4cad0",
            "7a66d33e350141c3b155858e07b25e9d",
            "743e3da6edab4465a8d1cdd6425d1c79",
            "de856c71db88436a83b80b6bcf14b94c",
            "606ccc5d90b24bc4a61acd8975855ad2",
            "793147be596f4996b36aaf20a4187747",
            "528fe40a390a44cdaf50adcdf9c0137a",
            "f1998a1537804cff88dad15df1d96a19",
            "37c0d3a03f5d46a99190371ec67684ed",
            "f1920d2a35c24b078b17765ffe2a857d",
            "0b8db95a8fb14c9fb5dfc717ea367b89",
            "4ccabe57898c41a5b134e566feb94171",
            "03a880517f7e4f17b47c6448bc8433dc",
            "d16f6a28bd6645e4b9035e7998845a93",
            "19147105768849cd88900195c791ac54",
            "c1b17cbfdddb450680d6e08022697603",
            "083da32680cb4424897ec8a299875036",
            "49fa4765f0e7489b821a826f7ae3edd9",
            "620573a527864c96be1d1694f307012a",
            "6d8f515efe7845dbabb2609b0a1aabe8",
            "8223ae5c41654eff8c5eed2827ec38fb",
            "a4201277bd45497c9784e749c87d2b38",
            "333fa5d1ce5b42c9908918cdaef65fbd",
            "986a46a7f0a746b4938c38e4a0f516f3",
            "eead2635042243ee8e9a3ec599271b88",
            "f1cffa1b4db84e108c150a43162e3158",
            "0cfd087b32e947cf88d152924e9c617c",
            "206a2583023d43b1936945ea30fa35a2",
            "f84a7d17b8dd4c109dcef37ecc210eeb",
            "53cf2819e47f450da74a9f999ffb866a",
            "2c89fed43426429fb50d82555986d53f",
            "96041941d66d433ea3f496cd197cb8c1",
            "082f8e8aa84f42c49b1f91f73fcd13fd",
            "23567a86036a48f9a84f2548960d367b",
            "7e3e22d827e241daadf788b2897ce0df"
          ]
        },
        "outputId": "9f25e386-935b-4fc0-a64d-b238ab5d63dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8dcbcb395d4e4db3b6f3a0120b3bbbb0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2598f35d31324e98bd1b17e92e201371"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/330 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72946030402e49198efef9f49b186250"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/593 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3db49867896940779f923152a47f24e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a50c7c6dade744849131d4aa84c71d48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ".gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a116fcc866c24ce58e8baf6e1b043306"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/534k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6853279eb03443ec87744367ce0fb426"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/6.85G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "318ffe24262d4bd8a30afad62ad2a433"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open wikitext-2-v1.zip, wikitext-2-v1.zip.zip or wikitext-2-v1.zip.ZIP.\n",
            "cat: wikitext-2/wikitext-2-v1/test.txt: No such file or directory\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78a0d80e763c489d86d303a26e871a71"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/685k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e903adcc63244140a5b5cf59cdbe7d06"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/6.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81e3b859aac04ba8b51b8d72eab668a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "validation-00000-of-00001.parquet:   0%|          | 0.00/618k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20e109cc704a413b855240f48d9a0045"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30ce1f504694467a95a02a1f5c490398"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "743e3da6edab4465a8d1cdd6425d1c79"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d16f6a28bd6645e4b9035e7998845a93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating CSV from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eead2635042243ee8e9a3ec599271b88"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1268938"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 4. Конвертация и квантование\n",
        "\n",
        "def convert_and_quantize():\n",
        "    # Конвертация в FP16\n",
        "    fp16_path = os.path.join(MODEL_PATH, \"ggml-model-f16.gguf\")\n",
        "    if not os.path.exists(fp16_path):\n",
        "        !python llama.cpp/convert_hf_to_gguf.py {MODEL_PATH} \\\n",
        "            --outfile {fp16_path} \\\n",
        "            --outtype f16\n",
        "\n",
        "    # Квантование для всех выбранных типов\n",
        "    for quant in QUANT_TYPES:\n",
        "        quant_path = os.path.join(MODEL_PATH, f\"ggml-model-{quant}.gguf\")\n",
        "        if not os.path.exists(quant_path):\n",
        "            !./llama.cpp/build/bin/llama-quantize {fp16_path} {quant_path} {quant}\n",
        "\n",
        "convert_and_quantize()"
      ],
      "metadata": {
        "id": "p_8Be26Y1gfs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "892552d2-75b8-42c6-8a67-94d2b0c92af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:hf-to-gguf:Loading model: open_llama_3B\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model.bin'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {3200, 32000}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F16, shape = {3200, 3200}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F16, shape = {8640, 3200}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F16, shape = {3200, 8640}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {3200}\n",
            "INFO:hf-to-gguf:output.weight,               torch.float16 --> F16, shape = {3200, 32000}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 2048\n",
            "INFO:hf-to-gguf:gguf: embedding length = 3200\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 8640\n",
            "INFO:hf-to-gguf:gguf: head count = 32\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:gguf.vocab:Setting special token type bos to 1\n",
            "INFO:gguf.vocab:Setting special token type eos to 2\n",
            "INFO:gguf.vocab:Setting special token type pad to 0\n",
            "INFO:gguf.vocab:Setting add_bos_token to True\n",
            "INFO:gguf.vocab:Setting add_eos_token to False\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/models/open_llama_3B/ggml-model-f16.gguf: n_tensors = 237, total_size = 6.9G\n",
            "Writing: 100% 6.85G/6.85G [00:12<00:00, 534Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/models/open_llama_3B/ggml-model-f16.gguf\n",
            "main: build = 5161 (66023048)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/models/open_llama_3B/ggml-model-f16.gguf' to '/content/models/open_llama_3B/ggml-model-q3_k.gguf' as Q3_K\n",
            "llama_model_loader: loaded meta data with 24 key-value pairs and 237 tensors from /content/models/open_llama_3B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = open_llama_3B\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 3.4B\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 26\n",
            "llama_model_loader: - kv   5:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 3200\n",
            "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 8640\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 100\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   53 tensors\n",
            "llama_model_loader: - type  f16:  184 tensors\n",
            "[   1/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q8_0 .. size =   195.31 MiB ->   103.76 MiB\n",
            "[   2/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   3/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 32000 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =   195.31 MiB ->    54.93 MiB\n",
            "[   4/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[   5/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   6/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[   7/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[   8/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[   9/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  10/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[  11/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  12/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[  13/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[  14/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  15/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  16/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[  17/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  18/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  19/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[  20/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  21/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[  22/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[  23/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  24/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  25/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[  26/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  27/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  28/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[  29/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  30/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[  31/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[  32/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  33/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  34/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[  35/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  36/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  37/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[  38/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  39/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[  40/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[  41/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  42/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  43/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[  44/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  45/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  46/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[  47/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  48/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[  49/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[  50/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  51/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  52/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[  53/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  54/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  55/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[  56/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  57/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[  58/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[  59/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  60/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  61/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[  62/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  63/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  64/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[  65/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  66/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[  67/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[  68/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  69/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  70/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[  71/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  72/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  73/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[  74/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  75/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[  76/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[  77/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  78/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  79/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[  80/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  81/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  82/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[  83/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  84/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[  85/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[  86/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  87/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  88/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[  89/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  90/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  91/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[  92/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  93/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[  94/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[  95/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  96/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  97/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[  98/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  99/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 100/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 101/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 102/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 103/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 104/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 105/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 106/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 107/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 108/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 109/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 110/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 111/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 112/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 113/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 114/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 115/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 116/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 117/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 118/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 119/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 120/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 121/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 122/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 123/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 124/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 125/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 126/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 127/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 128/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 129/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 130/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 131/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 132/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 133/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 134/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 135/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 136/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 137/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 138/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 139/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 140/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 141/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 142/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 143/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 144/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 145/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 146/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 147/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 148/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 149/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 150/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 151/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 152/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 153/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 154/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 155/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 156/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 157/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 158/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 159/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 160/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 161/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 162/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 163/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 164/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 165/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 166/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 167/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 168/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 169/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 170/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 171/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 172/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 173/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 174/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 175/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 176/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 177/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 178/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 179/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 180/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 181/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 182/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 183/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 184/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 185/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 186/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 187/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 188/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 189/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 190/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 191/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 192/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 193/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 194/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 195/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 196/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 197/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 198/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 199/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 200/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 201/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 202/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 203/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 204/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 205/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 206/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 207/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 208/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 209/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 210/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 211/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 212/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 213/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 214/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 215/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 216/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 217/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 218/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 219/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 220/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 221/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 222/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 223/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 224/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 225/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 226/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 227/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 228/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 229/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 230/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 231/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 232/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    19.53 MiB ->     5.49 MiB\n",
            "[ 233/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 234/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 235/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "[ 236/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 237/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q3_K - using fallback quantization iq4_nl\n",
            "converting to iq4_nl .. size =    52.73 MiB ->    14.83 MiB\n",
            "llama_model_quantize_impl: model size  =  6535.80 MB\n",
            "llama_model_quantize_impl: quant size  =  2039.53 MB\n",
            "llama_model_quantize_impl: WARNING: 183 of 184 tensor(s) required fallback quantization\n",
            "\n",
            "main: quantize time = 312597.34 ms\n",
            "main:    total time = 312597.34 ms\n",
            "main: build = 5161 (66023048)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/models/open_llama_3B/ggml-model-f16.gguf' to '/content/models/open_llama_3B/ggml-model-q4_k.gguf' as Q4_K\n",
            "llama_model_loader: loaded meta data with 24 key-value pairs and 237 tensors from /content/models/open_llama_3B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = open_llama_3B\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 3.4B\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 26\n",
            "llama_model_loader: - kv   5:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 3200\n",
            "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 8640\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 100\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   53 tensors\n",
            "llama_model_loader: - type  f16:  184 tensors\n",
            "[   1/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q8_0 .. size =   195.31 MiB ->   103.76 MiB\n",
            "[   2/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   3/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 32000 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =   195.31 MiB ->    67.14 MiB\n",
            "[   4/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[   5/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   6/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[   7/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[   8/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[   9/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  10/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  11/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  12/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  13/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  14/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  15/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  16/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  17/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  18/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  19/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  20/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  21/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  22/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  23/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  24/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  25/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  26/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  27/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  28/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  29/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  30/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  31/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  32/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  33/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  34/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  35/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  36/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  37/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  38/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  39/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  40/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  41/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  42/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  43/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  44/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  45/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  46/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  47/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  48/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  49/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  50/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  51/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  52/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  53/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  54/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  55/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  56/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  57/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  58/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  59/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  60/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  61/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  62/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  63/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  64/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  65/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  66/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  67/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  68/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  69/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  70/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  71/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  72/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  73/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  74/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  75/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  76/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  77/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  78/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  79/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  80/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  81/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  82/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  83/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  84/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  85/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  86/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  87/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  88/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  89/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  90/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  91/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  92/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  93/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[  94/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  95/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  96/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  97/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  98/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[  99/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 100/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 101/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 102/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 103/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 104/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 105/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 106/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 107/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 108/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 109/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 110/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 111/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 112/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 113/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 114/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 115/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 116/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 117/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 118/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 119/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 120/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 121/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 122/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 123/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 124/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 125/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 126/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 127/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 128/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 129/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 130/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 131/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 132/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 133/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 134/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 135/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 136/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 137/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 138/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 139/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 140/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 141/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 142/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 143/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 144/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 145/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 146/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 147/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 148/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 149/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 150/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 151/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 152/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 153/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 154/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 155/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 156/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 157/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 158/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 159/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 160/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 161/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 162/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 163/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 164/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 165/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 166/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 167/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 168/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 169/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 170/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 171/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 172/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 173/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 174/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 175/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 176/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 177/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 178/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 179/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 180/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 181/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 182/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 183/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 184/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 185/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 186/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 187/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 188/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 189/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 190/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 191/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 192/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 193/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 194/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 195/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 196/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 197/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 198/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 199/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 200/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 201/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 202/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 203/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 204/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 205/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 206/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 207/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 208/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 209/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 210/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 211/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 212/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 213/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 214/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 215/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 216/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 217/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 218/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 219/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 220/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 221/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 222/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 223/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 224/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 225/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 226/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 227/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 228/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 229/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 230/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 231/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 232/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB\n",
            "[ 233/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 234/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 235/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "[ 236/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 237/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB\n",
            "llama_model_quantize_impl: model size  =  6535.80 MB\n",
            "llama_model_quantize_impl: quant size  =  2459.88 MB\n",
            "llama_model_quantize_impl: WARNING: 183 of 184 tensor(s) required fallback quantization\n",
            "\n",
            "main: quantize time = 24004.61 ms\n",
            "main:    total time = 24004.61 ms\n",
            "main: build = 5161 (66023048)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/models/open_llama_3B/ggml-model-f16.gguf' to '/content/models/open_llama_3B/ggml-model-q5_k.gguf' as Q5_K\n",
            "llama_model_loader: loaded meta data with 24 key-value pairs and 237 tensors from /content/models/open_llama_3B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = open_llama_3B\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 3.4B\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 26\n",
            "llama_model_loader: - kv   5:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 3200\n",
            "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 8640\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 100\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   53 tensors\n",
            "llama_model_loader: - type  f16:  184 tensors\n",
            "[   1/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q8_0 .. size =   195.31 MiB ->   103.76 MiB\n",
            "[   2/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   3/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 32000 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =   195.31 MiB ->    73.24 MiB\n",
            "[   4/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[   5/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   6/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[   7/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[   8/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[   9/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  10/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  11/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  12/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  13/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  14/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  15/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  16/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  17/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  18/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  19/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  20/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  21/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  22/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  23/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  24/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  25/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  26/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  27/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  28/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  29/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  30/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  31/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  32/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  33/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  34/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  35/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  36/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  37/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  38/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  39/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  40/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  41/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  42/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  43/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  44/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  45/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  46/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  47/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  48/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  49/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  50/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  51/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  52/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  53/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  54/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  55/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  56/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  57/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  58/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  59/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  60/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  61/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  62/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  63/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  64/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  65/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  66/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  67/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  68/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  69/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  70/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  71/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  72/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  73/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  74/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  75/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  76/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  77/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  78/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  79/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  80/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  81/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  82/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  83/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  84/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  85/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  86/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  87/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  88/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  89/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  90/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  91/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  92/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  93/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[  94/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  95/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  96/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  97/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  98/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[  99/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 100/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 101/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 102/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 103/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 104/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 105/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 106/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 107/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 108/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 109/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 110/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 111/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 112/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 113/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 114/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 115/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 116/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 117/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 118/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 119/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 120/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 121/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 122/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 123/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 124/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 125/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 126/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 127/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 128/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 129/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 130/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 131/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 132/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 133/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 134/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 135/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 136/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 137/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 138/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 139/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 140/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 141/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 142/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 143/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 144/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 145/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 146/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 147/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 148/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 149/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 150/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 151/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 152/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 153/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 154/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 155/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 156/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 157/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 158/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 159/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 160/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 161/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 162/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 163/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 164/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 165/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 166/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 167/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 168/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 169/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 170/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 171/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 172/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 173/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 174/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 175/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 176/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 177/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 178/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 179/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 180/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 181/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 182/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 183/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 184/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 185/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 186/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 187/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 188/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 189/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 190/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 191/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 192/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 193/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 194/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 195/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 196/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 197/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 198/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 199/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 200/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 201/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 202/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 203/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 204/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 205/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 206/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 207/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 208/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 209/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 210/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 211/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 212/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 213/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 214/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 215/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 216/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 217/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 218/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 219/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 220/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 221/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 222/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 223/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 224/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 225/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 226/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 227/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 228/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 229/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 230/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 231/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 232/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB\n",
            "[ 233/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 234/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 8640 x 3200 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 235/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "[ 236/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 237/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 3200 x 8640 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB\n",
            "llama_model_quantize_impl: model size  =  6535.80 MB\n",
            "llama_model_quantize_impl: quant size  =  2628.64 MB\n",
            "llama_model_quantize_impl: WARNING: 183 of 184 tensor(s) required fallback quantization\n",
            "\n",
            "main: quantize time = 23916.81 ms\n",
            "main:    total time = 23916.81 ms\n",
            "main: build = 5161 (66023048)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/models/open_llama_3B/ggml-model-f16.gguf' to '/content/models/open_llama_3B/ggml-model-q8_0.gguf' as Q8_0\n",
            "llama_model_loader: loaded meta data with 24 key-value pairs and 237 tensors from /content/models/open_llama_3B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = open_llama_3B\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 3.4B\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 26\n",
            "llama_model_loader: - kv   5:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 3200\n",
            "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 8640\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 100\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   53 tensors\n",
            "llama_model_loader: - type  f16:  184 tensors\n",
            "[   1/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q8_0 .. size =   195.31 MiB ->   103.76 MiB\n",
            "[   2/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   3/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q8_0 .. size =   195.31 MiB ->   103.76 MiB\n",
            "[   4/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[   5/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   6/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[   7/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[   8/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[   9/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  10/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  11/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  12/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  13/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  14/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  15/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  16/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  17/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  18/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  19/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  20/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  21/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  22/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  23/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  24/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  25/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  26/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  27/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  28/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  29/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  30/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  31/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  32/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  33/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  34/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  35/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  36/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  37/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  38/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  39/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  40/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  41/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  42/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  43/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  44/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  45/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  46/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  47/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  48/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  49/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  50/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  51/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  52/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  53/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  54/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  55/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  56/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  57/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  58/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  59/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  60/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  61/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  62/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  63/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  64/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  65/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  66/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  67/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  68/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  69/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  70/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  71/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  72/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  73/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  74/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  75/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  76/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  77/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  78/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  79/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  80/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  81/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  82/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  83/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  84/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  85/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  86/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  87/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  88/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  89/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  90/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  91/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  92/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  93/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[  94/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  95/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  96/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  97/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  98/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[  99/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 100/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 101/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 102/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 103/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 104/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 105/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 106/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 107/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 108/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 109/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 110/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 111/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 112/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 113/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 114/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 115/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 116/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 117/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 118/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 119/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 120/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 121/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 122/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 123/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 124/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 125/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 126/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 127/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 128/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 129/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 130/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 131/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 132/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 133/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 134/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 135/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 136/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 137/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 138/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 139/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 140/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 141/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 142/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 143/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 144/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 145/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 146/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 147/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 148/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 149/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 150/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 151/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 152/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 153/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 154/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 155/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 156/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 157/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 158/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 159/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 160/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 161/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 162/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 163/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 164/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 165/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 166/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 167/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 168/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 169/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 170/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 171/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 172/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 173/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 174/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 175/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 176/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 177/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 178/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 179/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 180/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 181/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 182/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 183/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 184/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 185/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 186/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 187/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 188/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 189/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 190/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 191/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 192/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 193/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 194/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 195/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 196/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 197/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 198/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 199/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 200/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 201/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 202/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 203/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 204/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 205/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 206/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 207/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 208/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 209/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 210/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 211/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 212/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 213/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 214/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 215/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 216/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 217/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 218/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 219/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 220/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 221/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 222/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 223/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 224/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 225/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 226/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 227/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 228/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 229/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 230/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 231/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 232/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 233/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB\n",
            "[ 234/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 235/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "[ 236/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 237/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB\n",
            "llama_model_quantize_impl: model size  =  6535.80 MB\n",
            "llama_model_quantize_impl: quant size  =  3472.45 MB\n",
            "\n",
            "main: quantize time = 25633.28 ms\n",
            "main:    total time = 25633.28 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 5. Измерение перплексии\n",
        "\n",
        "results = []\n",
        "\n",
        "def run_perplexity_test(model_file, quant_type, ctx_length):\n",
        "    try:\n",
        "\n",
        "        output = !./llama.cpp/build/bin/llama-perplexity -m {model_file} -f \"/content/llama.cpp/prompts/mnemonics.txt\" -c {ctx_length}\n",
        "\n",
        "        ppl = None\n",
        "\n",
        "        for elem in output:\n",
        "            if \"PPL\" in elem:\n",
        "                ppl = elem.split(\" = \")[-1]\n",
        "                print(ppl)\n",
        "                break\n",
        "\n",
        "        if ppl is not None:\n",
        "            size_gb = os.path.getsize(model_file)/(1024**3)\n",
        "            return ppl, size_gb\n",
        "\n",
        "    except:\n",
        "        print(f\"Ошибка для {quant_type} (ctx {ctx_length})\")\n",
        "\n",
        "        return None, None\n",
        "\n",
        "# Запуск тестов для всех вариантов\n",
        "for quant in QUANT_TYPES + [\"f16\"]: #[\"f16\"] +\n",
        "    model_file = f\"/content/models/open_llama_{MODEL_SIZE}/ggml-model-{quant}.gguf\"\n",
        "\n",
        "    if not os.path.exists(model_file):\n",
        "        continue\n",
        "\n",
        "    for ctx in CONTEXT_LENGTHS:\n",
        "        print(f\"Тестируем {quant} (ctx {ctx})...\")\n",
        "\n",
        "        ppl, size = run_perplexity_test(model_file, quant, ctx)\n",
        "\n",
        "        if ppl:\n",
        "            results.append({\n",
        "                \"Model\": MODEL_SIZE,\n",
        "                \"Quant\": quant,\n",
        "                \"Context\": ctx,\n",
        "                \"Perplexity\": ppl,\n",
        "                \"Size_GB\": size\n",
        "            })\n",
        "\n",
        "# Сохранение результатов\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(\"results.csv\", index=False)\n",
        "df\n",
        "\n"
      ],
      "metadata": {
        "id": "KdtzLP9CL2XQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "f3d2e55b-c117-45b7-a2a3-40eae9ffecd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Тестируем q3_k (ctx 256)...\n",
            "5.7587 +/- 0.47326\n",
            "Тестируем q4_k (ctx 256)...\n",
            "5.7352 +/- 0.46775\n",
            "Тестируем q5_k (ctx 256)...\n",
            "5.7472 +/- 0.47020\n",
            "Тестируем q8_0 (ctx 256)...\n",
            "5.7250 +/- 0.46798\n",
            "Тестируем f16 (ctx 256)...\n",
            "5.7237 +/- 0.46791\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Model Quant Context          Perplexity   Size_GB\n",
              "0    3B  q3_k     256  5.7587 +/- 0.47326  1.992444\n",
              "1    3B  q4_k     256  5.7352 +/- 0.46775  2.402942\n",
              "2    3B  q5_k     256  5.7472 +/- 0.47020  2.567748\n",
              "3    3B  q8_0     256  5.7250 +/- 0.46798  3.391783\n",
              "4    3B   f16     256  5.7237 +/- 0.46791  6.383340"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d5c328f1-4775-452b-8306-b7dbe9e46b8f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Quant</th>\n",
              "      <th>Context</th>\n",
              "      <th>Perplexity</th>\n",
              "      <th>Size_GB</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3B</td>\n",
              "      <td>q3_k</td>\n",
              "      <td>256</td>\n",
              "      <td>5.7587 +/- 0.47326</td>\n",
              "      <td>1.992444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3B</td>\n",
              "      <td>q4_k</td>\n",
              "      <td>256</td>\n",
              "      <td>5.7352 +/- 0.46775</td>\n",
              "      <td>2.402942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3B</td>\n",
              "      <td>q5_k</td>\n",
              "      <td>256</td>\n",
              "      <td>5.7472 +/- 0.47020</td>\n",
              "      <td>2.567748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3B</td>\n",
              "      <td>q8_0</td>\n",
              "      <td>256</td>\n",
              "      <td>5.7250 +/- 0.46798</td>\n",
              "      <td>3.391783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3B</td>\n",
              "      <td>f16</td>\n",
              "      <td>256</td>\n",
              "      <td>5.7237 +/- 0.46791</td>\n",
              "      <td>6.383340</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5c328f1-4775-452b-8306-b7dbe9e46b8f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d5c328f1-4775-452b-8306-b7dbe9e46b8f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d5c328f1-4775-452b-8306-b7dbe9e46b8f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-52e5e754-3816-43fb-ab30-4444c71c14c1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-52e5e754-3816-43fb-ab30-4444c71c14c1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-52e5e754-3816-43fb-ab30-4444c71c14c1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"3B\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Quant\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"q4_k\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Context\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"256\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Perplexity\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"5.7352 +/- 0.46775\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size_GB\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.771627973608373,\n        \"min\": 1.9924444556236267,\n        \"max\": 6.383339822292328,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2.402941644191742\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}