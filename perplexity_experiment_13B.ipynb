{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8acdbb3ae32d45289de865e997f7e47a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c8d50a55ff249f69bb84d35342abfcb",
              "IPY_MODEL_2be4b68e981a4be59bd69a11a64a0095",
              "IPY_MODEL_d5f1431140f343bb8a2ee78ed31a54b6"
            ],
            "layout": "IPY_MODEL_55e4b1f6eed14c79a39cf5887e458bd6"
          }
        },
        "0c8d50a55ff249f69bb84d35342abfcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03abcb41b4404b52be920607116e9581",
            "placeholder": "​",
            "style": "IPY_MODEL_59ae507df8d4499e9132f643c140cdf0",
            "value": "Fetching 10 files: 100%"
          }
        },
        "2be4b68e981a4be59bd69a11a64a0095": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5af0b882d28c42bc881e4197fb206743",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e6937f0f00f43c6be36f4ef4a221030",
            "value": 10
          }
        },
        "d5f1431140f343bb8a2ee78ed31a54b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20ccc9d28dc24e73ae2c030e918b0637",
            "placeholder": "​",
            "style": "IPY_MODEL_621faa89609d479d83619000ff6080c8",
            "value": " 10/10 [00:08&lt;00:00,  1.35s/it]"
          }
        },
        "55e4b1f6eed14c79a39cf5887e458bd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03abcb41b4404b52be920607116e9581": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59ae507df8d4499e9132f643c140cdf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5af0b882d28c42bc881e4197fb206743": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e6937f0f00f43c6be36f4ef4a221030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "20ccc9d28dc24e73ae2c030e918b0637": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "621faa89609d479d83619000ff6080c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eaa7db080bc8420c87665efb725b6926": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab66df95d22f437193a2fb107e2655b3",
              "IPY_MODEL_f75d3d98e80b4d719cc8f3e40194974d",
              "IPY_MODEL_894c6329839f424ba746d482264f37ed"
            ],
            "layout": "IPY_MODEL_6104e34998c340b691959f79e88d49ba"
          }
        },
        "ab66df95d22f437193a2fb107e2655b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc7f34d8353747b8af7c0e481a4d28fe",
            "placeholder": "​",
            "style": "IPY_MODEL_c8268af69cb2439498111801bf282d9e",
            "value": "README.md: 100%"
          }
        },
        "f75d3d98e80b4d719cc8f3e40194974d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bce98a45dedc404c967d518555c73861",
            "max": 10464,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c8b02c83494349c4b833e585805bf3e9",
            "value": 10464
          }
        },
        "894c6329839f424ba746d482264f37ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e7958b7014e4f61b45d65c71b0cfba4",
            "placeholder": "​",
            "style": "IPY_MODEL_fafa03f5adbe4bad9b0183c34cdb6cb3",
            "value": " 10.5k/10.5k [00:00&lt;00:00, 1.18MB/s]"
          }
        },
        "6104e34998c340b691959f79e88d49ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc7f34d8353747b8af7c0e481a4d28fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8268af69cb2439498111801bf282d9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bce98a45dedc404c967d518555c73861": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8b02c83494349c4b833e585805bf3e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e7958b7014e4f61b45d65c71b0cfba4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fafa03f5adbe4bad9b0183c34cdb6cb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1929fa7ab70b4b669eaae29a3b20937d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aabb5234d1284df7b5282a02d609882a",
              "IPY_MODEL_5b2b26d51aff4f2496b71a033c73c532",
              "IPY_MODEL_6445e841a5374d34bdfcdd97dc62281d"
            ],
            "layout": "IPY_MODEL_70011cb721a84873bc9038f01780c1a5"
          }
        },
        "aabb5234d1284df7b5282a02d609882a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4db46f1f1c204169b5083b735dc8a08b",
            "placeholder": "​",
            "style": "IPY_MODEL_0d28b3573d5b45ea9c2a7f0614721570",
            "value": "test-00000-of-00001.parquet: 100%"
          }
        },
        "5b2b26d51aff4f2496b71a033c73c532": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40fd589d21b545c896d601625b95abec",
            "max": 685430,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c44f9acbd20476ea45f40aae44f8b74",
            "value": 685430
          }
        },
        "6445e841a5374d34bdfcdd97dc62281d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96cc32240d09438a819950c9de1a722b",
            "placeholder": "​",
            "style": "IPY_MODEL_1ac643adb24f45089aed019060dcc0a4",
            "value": " 685k/685k [00:00&lt;00:00, 9.76MB/s]"
          }
        },
        "70011cb721a84873bc9038f01780c1a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4db46f1f1c204169b5083b735dc8a08b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d28b3573d5b45ea9c2a7f0614721570": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40fd589d21b545c896d601625b95abec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c44f9acbd20476ea45f40aae44f8b74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96cc32240d09438a819950c9de1a722b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ac643adb24f45089aed019060dcc0a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd6a01c02aa94d769876cc3023071bb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32b0e00c3ad048fda93747b1ee3217c9",
              "IPY_MODEL_6b87f293ed7746b2b89542ceb1a20472",
              "IPY_MODEL_9b182ad2b1f54a65ae0d895f8ab8ccec"
            ],
            "layout": "IPY_MODEL_5ceafee5e19346ab803b3da12090ce9b"
          }
        },
        "32b0e00c3ad048fda93747b1ee3217c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9a4bd0bd58d40a0af9643124d835d88",
            "placeholder": "​",
            "style": "IPY_MODEL_16212f190a174c5f8a5d335058e2dacd",
            "value": "train-00000-of-00001.parquet: 100%"
          }
        },
        "6b87f293ed7746b2b89542ceb1a20472": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77e5554fa08342d497058a202425f247",
            "max": 6068114,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_792bceffa6784a09a3149e045fdd3a13",
            "value": 6068114
          }
        },
        "9b182ad2b1f54a65ae0d895f8ab8ccec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0134348bb19c4815a5b2fe459b5c23ec",
            "placeholder": "​",
            "style": "IPY_MODEL_ac0998d968bf420f8b08c9865b72ce2f",
            "value": " 6.07M/6.07M [00:00&lt;00:00, 127MB/s]"
          }
        },
        "5ceafee5e19346ab803b3da12090ce9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9a4bd0bd58d40a0af9643124d835d88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16212f190a174c5f8a5d335058e2dacd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77e5554fa08342d497058a202425f247": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "792bceffa6784a09a3149e045fdd3a13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0134348bb19c4815a5b2fe459b5c23ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac0998d968bf420f8b08c9865b72ce2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6c69bbc47ac46cab99c96326672eec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_081c3935f631458ca7d003cc3d0cde50",
              "IPY_MODEL_662bc92f81c44685b310a944058517a9",
              "IPY_MODEL_0085d7cca945467fb5250ccc5327665a"
            ],
            "layout": "IPY_MODEL_8d1ae84a02a34d3583ef932b2921c5f8"
          }
        },
        "081c3935f631458ca7d003cc3d0cde50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a955f0ac5425481a876d680eb75587db",
            "placeholder": "​",
            "style": "IPY_MODEL_c31c42df7420483e8b7f61787b3cc341",
            "value": "validation-00000-of-00001.parquet: 100%"
          }
        },
        "662bc92f81c44685b310a944058517a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eea431b8e91f4262a617b6a123b7cbd2",
            "max": 617738,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_24ef9153ee4848b8b940275da5569371",
            "value": 617738
          }
        },
        "0085d7cca945467fb5250ccc5327665a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8dcc5263e304d2c8aa57fd8b8cb0cbe",
            "placeholder": "​",
            "style": "IPY_MODEL_61d91f2c96a944a4909289b5b068bd1e",
            "value": " 618k/618k [00:00&lt;00:00, 76.0MB/s]"
          }
        },
        "8d1ae84a02a34d3583ef932b2921c5f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a955f0ac5425481a876d680eb75587db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c31c42df7420483e8b7f61787b3cc341": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eea431b8e91f4262a617b6a123b7cbd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24ef9153ee4848b8b940275da5569371": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f8dcc5263e304d2c8aa57fd8b8cb0cbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61d91f2c96a944a4909289b5b068bd1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80adadd0076446abb4cfd248c2a03e83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0790ec5e641e4e708e8a380bc9865864",
              "IPY_MODEL_a8450d0def20453b871fc12fd9b35134",
              "IPY_MODEL_a80d4d5957a44aa8ba904018b27b5f65"
            ],
            "layout": "IPY_MODEL_c79996d9dc694c7a9fcb99996daaae62"
          }
        },
        "0790ec5e641e4e708e8a380bc9865864": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98a6c698b84c4a3d99c31a2adfa40ece",
            "placeholder": "​",
            "style": "IPY_MODEL_01c0b037e7274e76b94116c1f4d3f2d6",
            "value": "Generating test split: 100%"
          }
        },
        "a8450d0def20453b871fc12fd9b35134": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0329a20df2be4904914b390beb3beb07",
            "max": 4358,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c692b94c5ac4431a0a2ef8f237c3e59",
            "value": 4358
          }
        },
        "a80d4d5957a44aa8ba904018b27b5f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bcb2e35f8bc44bc9d63f6aa3b34a18c",
            "placeholder": "​",
            "style": "IPY_MODEL_1a96aec258484d4f908bb24587230065",
            "value": " 4358/4358 [00:00&lt;00:00, 231175.01 examples/s]"
          }
        },
        "c79996d9dc694c7a9fcb99996daaae62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98a6c698b84c4a3d99c31a2adfa40ece": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01c0b037e7274e76b94116c1f4d3f2d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0329a20df2be4904914b390beb3beb07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c692b94c5ac4431a0a2ef8f237c3e59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2bcb2e35f8bc44bc9d63f6aa3b34a18c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a96aec258484d4f908bb24587230065": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89dd761c16ab48088884f54c015d65cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d5d7c033c40e43f3b5f0cadd9598cda2",
              "IPY_MODEL_2f007079cda9408cba908bfb8a541f2f",
              "IPY_MODEL_c5417fcfddad4b6ebd34b5ce1b0785d0"
            ],
            "layout": "IPY_MODEL_caed50e36ea14629b917411aa452192b"
          }
        },
        "d5d7c033c40e43f3b5f0cadd9598cda2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ac27422c0274f20b308442da68675f7",
            "placeholder": "​",
            "style": "IPY_MODEL_2379211d1fd74272808b16e4b6ff1fd5",
            "value": "Generating train split: 100%"
          }
        },
        "2f007079cda9408cba908bfb8a541f2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2303bb3d5ddf4f2ab92aa566addd13b4",
            "max": 36718,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe40ea2a069e4f8c8e349f12266cfb92",
            "value": 36718
          }
        },
        "c5417fcfddad4b6ebd34b5ce1b0785d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51889f2fdfac4e928d7e0fa9bab6822c",
            "placeholder": "​",
            "style": "IPY_MODEL_7e8a0c936c3240ff8a9dbbff4ff1aecb",
            "value": " 36718/36718 [00:00&lt;00:00, 697347.72 examples/s]"
          }
        },
        "caed50e36ea14629b917411aa452192b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ac27422c0274f20b308442da68675f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2379211d1fd74272808b16e4b6ff1fd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2303bb3d5ddf4f2ab92aa566addd13b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe40ea2a069e4f8c8e349f12266cfb92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "51889f2fdfac4e928d7e0fa9bab6822c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e8a0c936c3240ff8a9dbbff4ff1aecb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07f125bcac804c32a1673e6fd1bc6aed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_771f10b01e724229aa402631b314df8b",
              "IPY_MODEL_7d39e26bc419480382afd2acfe6ccdbf",
              "IPY_MODEL_d582502de47b47f586d63fb399edac11"
            ],
            "layout": "IPY_MODEL_3759ab1180754f91adcb99718870d29b"
          }
        },
        "771f10b01e724229aa402631b314df8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18cf32e61de24071a09865ffe3cf657d",
            "placeholder": "​",
            "style": "IPY_MODEL_76cca21dfef04acd9cc3ed0ac28701ad",
            "value": "Generating validation split: 100%"
          }
        },
        "7d39e26bc419480382afd2acfe6ccdbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a8dd8143bb94fb489f4726c25ca1cac",
            "max": 3760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58fcafb8b5b54521a6304bc5c7c49802",
            "value": 3760
          }
        },
        "d582502de47b47f586d63fb399edac11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3d7b992d52d42b5bce739bbcfe389c0",
            "placeholder": "​",
            "style": "IPY_MODEL_66c4e0d00d5649919f402e1ca7bce452",
            "value": " 3760/3760 [00:00&lt;00:00, 265355.08 examples/s]"
          }
        },
        "3759ab1180754f91adcb99718870d29b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18cf32e61de24071a09865ffe3cf657d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76cca21dfef04acd9cc3ed0ac28701ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a8dd8143bb94fb489f4726c25ca1cac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58fcafb8b5b54521a6304bc5c7c49802": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d3d7b992d52d42b5bce739bbcfe389c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66c4e0d00d5649919f402e1ca7bce452": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc9a88d12cb941b9819c707b987ebe95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4f343e7f281240f299cf63571b6b16e1",
              "IPY_MODEL_5943799b8aa2424bab55caf66fd136fd",
              "IPY_MODEL_7e62802278d6470ca901288bcf670a1a"
            ],
            "layout": "IPY_MODEL_d5fca60f92ab438aaa7a80ae3ec27733"
          }
        },
        "4f343e7f281240f299cf63571b6b16e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c91aceb76094f209f3bb3a6eb70d9eb",
            "placeholder": "​",
            "style": "IPY_MODEL_8a4ccb403eda4774a68e9c101171e709",
            "value": "Creating CSV from Arrow format: 100%"
          }
        },
        "5943799b8aa2424bab55caf66fd136fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7d9e962ca6448769b98197b27b20332",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d65b5ea1ca94675a9995e6f361d9076",
            "value": 5
          }
        },
        "7e62802278d6470ca901288bcf670a1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f54246342d4472281bc7f6320028df2",
            "placeholder": "​",
            "style": "IPY_MODEL_51e07738151f41ce866c08edda43fa1d",
            "value": " 5/5 [00:00&lt;00:00,  5.17ba/s]"
          }
        },
        "d5fca60f92ab438aaa7a80ae3ec27733": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c91aceb76094f209f3bb3a6eb70d9eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a4ccb403eda4774a68e9c101171e709": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7d9e962ca6448769b98197b27b20332": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d65b5ea1ca94675a9995e6f361d9076": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f54246342d4472281bc7f6320028df2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51e07738151f41ce866c08edda43fa1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ## 1. Установка зависимостей и компиляция llama.cpp\n",
        "\n",
        "# %%\n",
        "# Установка системных зависимостей\n",
        "!apt-get update\n",
        "!apt-get install -y build-essential python3-pip\n",
        "!pip install huggingface-hub pandas matplotlib seaborn datasets\n",
        "\n",
        "# Компиляция llama.cpp\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "%cd llama.cpp\n",
        "!mkdir -p build\n",
        "%cd build\n",
        "!cmake ..\n",
        "!cmake --build . --config Release\n",
        "%cd ../..\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKJN9ta607_i",
        "outputId": "e39204de-5a93-4d20-e700-a86c1afaca1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:7 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [75.2 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,842 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,140 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,542 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,696 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,788 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,243 kB]\n",
            "Fetched 20.6 MB in 2s (11.0 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "The following additional packages will be installed:\n",
            "  python3-setuptools python3-wheel\n",
            "Suggested packages:\n",
            "  python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  python3-pip python3-setuptools python3-wheel\n",
            "0 upgraded, 3 newly installed, 0 to remove and 6 not upgraded.\n",
            "Need to get 1,677 kB of archives.\n",
            "After this operation, 8,968 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-setuptools all 59.6.0-1.2ubuntu0.22.04.2 [340 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-wheel all 0.37.1-2ubuntu0.22.04.1 [32.0 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-pip all 22.0.2+dfsg-1ubuntu0.5 [1,306 kB]\n",
            "Fetched 1,677 kB in 0s (5,068 kB/s)\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "(Reading database ... 122175 files and directories currently installed.)\n",
            "Preparing to unpack .../python3-setuptools_59.6.0-1.2ubuntu0.22.04.2_all.deb ...\n",
            "Unpacking python3-setuptools (59.6.0-1.2ubuntu0.22.04.2) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../python3-wheel_0.37.1-2ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../python3-pip_22.0.2+dfsg-1ubuntu0.5_all.deb ...\n",
            "Unpacking python3-pip (22.0.2+dfsg-1ubuntu0.5) ...\n",
            "Setting up python3-setuptools (59.6.0-1.2ubuntu0.22.04.2) ...\n",
            "Setting up python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Setting up python3-pip (22.0.2+dfsg-1ubuntu0.5) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.13.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2025.1.31)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "Successfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 48952, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 48952 (delta 33), reused 12 (delta 11), pack-reused 48879 (from 2)\u001b[K\n",
            "Receiving objects: 100% (48952/48952), 102.52 MiB | 36.97 MiB/s, done.\n",
            "Resolving deltas: 100% (35277/35277), done.\n",
            "/content/llama.cpp\n",
            "/content/llama.cpp/build\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\") \n",
            "-- Looking for pthread.h\n",
            "-- Looking for pthread.h - found\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE  \n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- Including CPU backend\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP: TRUE (found version \"4.5\")  \n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")  \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
            "[  4%] Built target ggml-base\n",
            "[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
            "[ 10%] Built target ggml-cpu\n",
            "[ 11%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
            "[ 11%] Built target ggml\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
            "[ 21%] Built target llama\n",
            "[ 21%] \u001b[34m\u001b[1mGenerating build details from Git\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\") \n",
            "[ 21%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[ 21%] Built target build_info\n",
            "[ 22%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 26%] Built target common\n",
            "[ 27%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 27%] Built target test-tokenizer-0\n",
            "[ 27%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 28%] Built target test-sampling\n",
            "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 30%] Built target test-grammar-parser\n",
            "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
            "[ 31%] Built target test-grammar-integration\n",
            "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 32%] Built target test-llama-grammar\n",
            "[ 32%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
            "[ 33%] Built target test-chat\n",
            "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
            "[ 34%] Built target test-json-schema-to-grammar\n",
            "[ 35%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 35%] Built target test-tokenizer-1-bpe\n",
            "[ 36%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
            "[ 36%] Built target test-tokenizer-1-spm\n",
            "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
            "[ 38%] Built target test-log\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
            "[ 39%] Built target test-chat-template\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
            "[ 41%] Built target test-arg-parser\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
            "[ 43%] Built target test-gguf\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[ 45%] Built target test-backend-ops\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
            "[ 46%] Built target test-model-load-cancel\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
            "[ 47%] Built target test-autorelease\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
            "[ 48%] Built target test-barrier\n",
            "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 49%] Built target test-quantize-fns\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 51%] Built target test-quantize-perf\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 52%] Built target test-rope\n",
            "[ 52%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
            "[ 53%] Built target test-c\n",
            "[ 53%] \u001b[32mBuilding CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
            "[ 54%] Built target llama-batched-bench\n",
            "[ 55%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
            "[ 55%] Built target llama-batched\n",
            "[ 56%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
            "[ 56%] Built target llama-embedding\n",
            "[ 57%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
            "[ 57%] Built target llama-eval-callback\n",
            "[ 58%] \u001b[32mBuilding CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gbnf-validator\u001b[0m\n",
            "[ 58%] Built target llama-gbnf-validator\n",
            "[ 58%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
            "[ 58%] Built target sha256\n",
            "[ 59%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
            "[ 59%] Built target xxhash\n",
            "[ 59%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
            "[ 59%] Built target sha1\n",
            "[ 59%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
            "[ 60%] Built target llama-gguf-hash\n",
            "[ 60%] \u001b[32mBuilding CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
            "[ 61%] Built target llama-gguf-split\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
            "[ 62%] Built target llama-gguf\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gritlm\u001b[0m\n",
            "[ 62%] Built target llama-gritlm\n",
            "[ 63%] \u001b[32mBuilding CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
            "[ 63%] Built target llama-imatrix\n",
            "[ 64%] \u001b[32mBuilding CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-infill\u001b[0m\n",
            "[ 64%] Built target llama-infill\n",
            "[ 64%] \u001b[32mBuilding CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[ 65%] Built target llama-bench\n",
            "[ 66%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
            "[ 66%] Built target llama-lookahead\n",
            "[ 66%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
            "[ 67%] Built target llama-lookup\n",
            "[ 67%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
            "[ 68%] Built target llama-lookup-create\n",
            "[ 68%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
            "[ 69%] Built target llama-lookup-merge\n",
            "[ 69%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
            "[ 70%] Built target llama-lookup-stats\n",
            "[ 70%] \u001b[32mBuilding CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
            "[ 71%] Built target llama-cli\n",
            "[ 72%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
            "[ 72%] Built target llama-parallel\n",
            "[ 73%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
            "[ 73%] Built target llama-passkey\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
            "[ 74%] Built target llama-perplexity\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[ 75%] Built target llama-quantize\n",
            "[ 75%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
            "[ 76%] Built target llama-retrieval\n",
            "[ 77%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
            "[ 77%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
            "[ 78%] Built target llama-server\n",
            "[ 78%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
            "[ 79%] Built target llama-save-load-state\n",
            "[ 79%] \u001b[32mBuilding CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\n",
            "[ 80%] Built target llama-run\n",
            "[ 80%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
            "[ 81%] Built target llama-simple\n",
            "[ 81%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
            "[ 81%] Built target llama-simple-chat\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
            "[ 82%] Built target llama-speculative\n",
            "[ 83%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
            "[ 83%] Built target llama-speculative-simple\n",
            "[ 84%] \u001b[32mBuilding CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
            "[ 84%] Built target llama-tokenize\n",
            "[ 85%] \u001b[32mBuilding CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
            "[ 85%] Built target llama-tts\n",
            "[ 85%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
            "[ 86%] Built target llama-gen-docs\n",
            "[ 86%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
            "[ 87%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 87%] \u001b[32mBuilding CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
            "[ 87%] Built target llama-cvector-generator\n",
            "[ 88%] \u001b[32mBuilding CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
            "[ 88%] Built target llama-export-lora\n",
            "[ 88%] \u001b[32mBuilding CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize-stats\u001b[0m\n",
            "[ 89%] Built target llama-quantize-stats\n",
            "[ 89%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o\u001b[0m\n",
            "[ 90%] Built target llava\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX static library libllava_static.a\u001b[0m\n",
            "[ 91%] Built target llava_static\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libllava_shared.so\u001b[0m\n",
            "[ 91%] Built target llava_shared\n",
            "[ 91%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n",
            "[ 92%] Built target mtmd\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX static library libmtmd_static.a\u001b[0m\n",
            "[ 93%] Built target mtmd_static\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd_shared.so\u001b[0m\n",
            "[ 93%] Built target mtmd_shared\n",
            "[ 94%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
            "[ 94%] Built target llama-llava-cli\n",
            "[ 95%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
            "[ 95%] Built target llama-gemma3-cli\n",
            "[ 95%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
            "[ 96%] Built target llama-minicpmv-cli\n",
            "[ 96%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
            "[ 97%] Built target llama-qwen2vl-cli\n",
            "[ 97%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-mtmd-cli\u001b[0m\n",
            "[ 97%] Built target llama-mtmd-cli\n",
            "[ 98%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-clip-quantize-cli\u001b[0m\n",
            "[ 98%] Built target llama-llava-clip-quantize-cli\n",
            "[ 99%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
            "[ 99%] Built target llama-vdot\n",
            "[100%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
            "[100%] Built target llama-q8dot\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 2. Настройка параметров эксперимента\n",
        "\n",
        "# %%\n",
        "import os\n",
        "from huggingface_hub import snapshot_download\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Конфигурация эксперимента\n",
        "MODEL_SIZE = \"13B\" #\"3B\" \"7B\" \"13B\"\n",
        "CONTEXT_LENGTHS = [\"256\"]  # Длины контекста для теста\n",
        "QUANT_TYPES = [\"q3_k\", \"q4_k\", \"q5_k\", \"q8_0\"]\n",
        "\n",
        "\n",
        "BASE_DIR = \"/content\"\n",
        "MODEL_NAME = f\"open_llama_{MODEL_SIZE}\"\n",
        "MODEL_PATH = os.path.join(BASE_DIR, \"models\", MODEL_NAME)\n",
        "DATA_FILE = os.path.join(BASE_DIR, \"test.txt\")\n",
        "\n",
        "# Создаем директории\n",
        "os.makedirs(MODEL_PATH, exist_ok=True)"
      ],
      "metadata": {
        "id": "2WaXTNRU091m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 3. Загрузка модели и данных\n",
        "\n",
        "# %%\n",
        "# Загрузка модели с Hugging Face Hub\n",
        "snapshot_download(\n",
        "    repo_id=f\"openlm-research/{MODEL_NAME}\",\n",
        "    local_dir=MODEL_PATH,\n",
        "    ignore_patterns=[\"*.txt\", \"*.md\", \"*.safetensors\", \"*.h5\", \"*.ot\"],\n",
        "    resume_download=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "8QV5g3kn1WGT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460,
          "referenced_widgets": [
            "8acdbb3ae32d45289de865e997f7e47a",
            "0c8d50a55ff249f69bb84d35342abfcb",
            "2be4b68e981a4be59bd69a11a64a0095",
            "d5f1431140f343bb8a2ee78ed31a54b6",
            "55e4b1f6eed14c79a39cf5887e458bd6",
            "03abcb41b4404b52be920607116e9581",
            "59ae507df8d4499e9132f643c140cdf0",
            "5af0b882d28c42bc881e4197fb206743",
            "8e6937f0f00f43c6be36f4ef4a221030",
            "20ccc9d28dc24e73ae2c030e918b0637",
            "621faa89609d479d83619000ff6080c8",
            "eaa7db080bc8420c87665efb725b6926",
            "ab66df95d22f437193a2fb107e2655b3",
            "f75d3d98e80b4d719cc8f3e40194974d",
            "894c6329839f424ba746d482264f37ed",
            "6104e34998c340b691959f79e88d49ba",
            "cc7f34d8353747b8af7c0e481a4d28fe",
            "c8268af69cb2439498111801bf282d9e",
            "bce98a45dedc404c967d518555c73861",
            "c8b02c83494349c4b833e585805bf3e9",
            "7e7958b7014e4f61b45d65c71b0cfba4",
            "fafa03f5adbe4bad9b0183c34cdb6cb3",
            "1929fa7ab70b4b669eaae29a3b20937d",
            "aabb5234d1284df7b5282a02d609882a",
            "5b2b26d51aff4f2496b71a033c73c532",
            "6445e841a5374d34bdfcdd97dc62281d",
            "70011cb721a84873bc9038f01780c1a5",
            "4db46f1f1c204169b5083b735dc8a08b",
            "0d28b3573d5b45ea9c2a7f0614721570",
            "40fd589d21b545c896d601625b95abec",
            "9c44f9acbd20476ea45f40aae44f8b74",
            "96cc32240d09438a819950c9de1a722b",
            "1ac643adb24f45089aed019060dcc0a4",
            "bd6a01c02aa94d769876cc3023071bb1",
            "32b0e00c3ad048fda93747b1ee3217c9",
            "6b87f293ed7746b2b89542ceb1a20472",
            "9b182ad2b1f54a65ae0d895f8ab8ccec",
            "5ceafee5e19346ab803b3da12090ce9b",
            "f9a4bd0bd58d40a0af9643124d835d88",
            "16212f190a174c5f8a5d335058e2dacd",
            "77e5554fa08342d497058a202425f247",
            "792bceffa6784a09a3149e045fdd3a13",
            "0134348bb19c4815a5b2fe459b5c23ec",
            "ac0998d968bf420f8b08c9865b72ce2f",
            "b6c69bbc47ac46cab99c96326672eec2",
            "081c3935f631458ca7d003cc3d0cde50",
            "662bc92f81c44685b310a944058517a9",
            "0085d7cca945467fb5250ccc5327665a",
            "8d1ae84a02a34d3583ef932b2921c5f8",
            "a955f0ac5425481a876d680eb75587db",
            "c31c42df7420483e8b7f61787b3cc341",
            "eea431b8e91f4262a617b6a123b7cbd2",
            "24ef9153ee4848b8b940275da5569371",
            "f8dcc5263e304d2c8aa57fd8b8cb0cbe",
            "61d91f2c96a944a4909289b5b068bd1e",
            "80adadd0076446abb4cfd248c2a03e83",
            "0790ec5e641e4e708e8a380bc9865864",
            "a8450d0def20453b871fc12fd9b35134",
            "a80d4d5957a44aa8ba904018b27b5f65",
            "c79996d9dc694c7a9fcb99996daaae62",
            "98a6c698b84c4a3d99c31a2adfa40ece",
            "01c0b037e7274e76b94116c1f4d3f2d6",
            "0329a20df2be4904914b390beb3beb07",
            "2c692b94c5ac4431a0a2ef8f237c3e59",
            "2bcb2e35f8bc44bc9d63f6aa3b34a18c",
            "1a96aec258484d4f908bb24587230065",
            "89dd761c16ab48088884f54c015d65cf",
            "d5d7c033c40e43f3b5f0cadd9598cda2",
            "2f007079cda9408cba908bfb8a541f2f",
            "c5417fcfddad4b6ebd34b5ce1b0785d0",
            "caed50e36ea14629b917411aa452192b",
            "1ac27422c0274f20b308442da68675f7",
            "2379211d1fd74272808b16e4b6ff1fd5",
            "2303bb3d5ddf4f2ab92aa566addd13b4",
            "fe40ea2a069e4f8c8e349f12266cfb92",
            "51889f2fdfac4e928d7e0fa9bab6822c",
            "7e8a0c936c3240ff8a9dbbff4ff1aecb",
            "07f125bcac804c32a1673e6fd1bc6aed",
            "771f10b01e724229aa402631b314df8b",
            "7d39e26bc419480382afd2acfe6ccdbf",
            "d582502de47b47f586d63fb399edac11",
            "3759ab1180754f91adcb99718870d29b",
            "18cf32e61de24071a09865ffe3cf657d",
            "76cca21dfef04acd9cc3ed0ac28701ad",
            "3a8dd8143bb94fb489f4726c25ca1cac",
            "58fcafb8b5b54521a6304bc5c7c49802",
            "d3d7b992d52d42b5bce739bbcfe389c0",
            "66c4e0d00d5649919f402e1ca7bce452",
            "fc9a88d12cb941b9819c707b987ebe95",
            "4f343e7f281240f299cf63571b6b16e1",
            "5943799b8aa2424bab55caf66fd136fd",
            "7e62802278d6470ca901288bcf670a1a",
            "d5fca60f92ab438aaa7a80ae3ec27733",
            "0c91aceb76094f209f3bb3a6eb70d9eb",
            "8a4ccb403eda4774a68e9c101171e709",
            "e7d9e962ca6448769b98197b27b20332",
            "0d65b5ea1ca94675a9995e6f361d9076",
            "7f54246342d4472281bc7f6320028df2",
            "51e07738151f41ce866c08edda43fa1d"
          ]
        },
        "outputId": "733b016a-f377-4107-ac67-c8c9c0a72967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8acdbb3ae32d45289de865e997f7e47a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open wikitext-2-v1.zip, wikitext-2-v1.zip.zip or wikitext-2-v1.zip.ZIP.\n",
            "cat: wikitext-2/wikitext-2-v1/test.txt: No such file or directory\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eaa7db080bc8420c87665efb725b6926"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/685k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1929fa7ab70b4b669eaae29a3b20937d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/6.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd6a01c02aa94d769876cc3023071bb1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "validation-00000-of-00001.parquet:   0%|          | 0.00/618k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6c69bbc47ac46cab99c96326672eec2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80adadd0076446abb4cfd248c2a03e83"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89dd761c16ab48088884f54c015d65cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07f125bcac804c32a1673e6fd1bc6aed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating CSV from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc9a88d12cb941b9819c707b987ebe95"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1268938"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 4. Конвертация и квантование\n",
        "\n",
        "# %%\n",
        "def convert_and_quantize():\n",
        "    # Конвертация в FP16\n",
        "    fp16_path = os.path.join(MODEL_PATH, \"ggml-model-f16.gguf\")\n",
        "    if not os.path.exists(fp16_path):\n",
        "        !python llama.cpp/convert_hf_to_gguf.py {MODEL_PATH} \\\n",
        "            --outfile {fp16_path} \\\n",
        "            --outtype f16\n",
        "\n",
        "    # Квантование для всех выбранных типов\n",
        "    for quant in QUANT_TYPES:\n",
        "        quant_path = os.path.join(MODEL_PATH, f\"ggml-model-{quant}.gguf\")\n",
        "        if not os.path.exists(quant_path):\n",
        "            !./llama.cpp/build/bin/llama-quantize {fp16_path} {quant_path} {quant}\n",
        "\n",
        "convert_and_quantize()"
      ],
      "metadata": {
        "id": "p_8Be26Y1gfs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c7f127d-cc64-48a4-a5cd-8a5741b3925b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:hf-to-gguf:Loading model: open_llama_13B\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00003.bin'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {5120, 32000}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00003.bin'\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00003-of-00003.bin'\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.32.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.32.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.32.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.32.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.32.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.32.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.32.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.32.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.32.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.33.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.33.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.33.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.33.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.33.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.33.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.33.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.33.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.33.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.34.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.34.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.34.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.34.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.34.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.34.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.34.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.34.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.34.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.35.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.35.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.35.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.35.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.35.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.35.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.35.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.35.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.35.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.36.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.36.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.36.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.36.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.36.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.36.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.36.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.36.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.36.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.37.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.37.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.37.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.37.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.37.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.37.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.37.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.37.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.37.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.38.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.38.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.38.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.38.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.38.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.38.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.38.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.38.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.38.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.39.attn_q.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.39.attn_k.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.39.attn_v.weight,        torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.39.attn_output.weight,   torch.float16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.39.ffn_gate.weight,      torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.39.ffn_down.weight,      torch.float16 --> F16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.39.ffn_up.weight,        torch.float16 --> F16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.39.attn_norm.weight,     torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.39.ffn_norm.weight,      torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:output.weight,               torch.float16 --> F16, shape = {5120, 32000}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 2048\n",
            "INFO:hf-to-gguf:gguf: embedding length = 5120\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 13824\n",
            "INFO:hf-to-gguf:gguf: head count = 40\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:gguf.vocab:Setting special token type bos to 1\n",
            "INFO:gguf.vocab:Setting special token type eos to 2\n",
            "INFO:gguf.vocab:Setting special token type pad to 0\n",
            "INFO:gguf.vocab:Setting add_bos_token to True\n",
            "INFO:gguf.vocab:Setting add_eos_token to False\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/models/open_llama_13B/ggml-model-f16.gguf: n_tensors = 363, total_size = 26.0G\n",
            "Writing: 100% 26.0G/26.0G [14:14<00:00, 30.5Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/models/open_llama_13B/ggml-model-f16.gguf\n",
            "main: build = 5165 (1d735c0b)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/models/open_llama_13B/ggml-model-f16.gguf' to '/content/models/open_llama_13B/ggml-model-q3_k.gguf' as Q3_K\n",
            "llama_model_loader: loaded meta data with 24 key-value pairs and 363 tensors from /content/models/open_llama_13B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = open_llama_13B\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 13B\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
            "llama_model_loader: - kv   5:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 13824\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - type  f32:   81 tensors\n",
            "llama_model_loader: - type  f16:  282 tensors\n",
            "[   1/ 363]                        output.weight - [ 5120, 32000,     1,     1], type =    f16, converting to q6_K .. size =   312.50 MiB ->   128.17 MiB\n",
            "[   2/ 363]                   output_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[   3/ 363]                    token_embd.weight - [ 5120, 32000,     1,     1], type =    f16, converting to q3_K .. size =   312.50 MiB ->    67.14 MiB\n",
            "[   4/ 363]                  blk.0.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[   5/ 363]               blk.0.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[   6/ 363]             blk.0.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[   7/ 363]                  blk.0.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[   8/ 363]                  blk.0.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[   9/ 363]                blk.0.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  10/ 363]                blk.0.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[  11/ 363]                blk.0.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  12/ 363]                  blk.0.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[  13/ 363]                  blk.1.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[  14/ 363]               blk.1.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  15/ 363]             blk.1.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  16/ 363]                  blk.1.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[  17/ 363]                  blk.1.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  18/ 363]                blk.1.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  19/ 363]                blk.1.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[  20/ 363]                blk.1.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  21/ 363]                  blk.1.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[  22/ 363]                  blk.2.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[  23/ 363]               blk.2.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  24/ 363]             blk.2.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  25/ 363]                  blk.2.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[  26/ 363]                  blk.2.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  27/ 363]                blk.2.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  28/ 363]                blk.2.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[  29/ 363]                blk.2.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  30/ 363]                  blk.2.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[  31/ 363]                  blk.3.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[  32/ 363]               blk.3.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  33/ 363]             blk.3.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  34/ 363]                  blk.3.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[  35/ 363]                  blk.3.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  36/ 363]                blk.3.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  37/ 363]                blk.3.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[  38/ 363]                blk.3.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  39/ 363]                  blk.3.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[  40/ 363]                  blk.4.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[  41/ 363]               blk.4.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  42/ 363]             blk.4.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  43/ 363]                  blk.4.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[  44/ 363]                  blk.4.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  45/ 363]                blk.4.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  46/ 363]                blk.4.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[  47/ 363]                blk.4.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  48/ 363]                  blk.4.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[  49/ 363]                  blk.5.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[  50/ 363]               blk.5.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  51/ 363]             blk.5.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  52/ 363]                  blk.5.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[  53/ 363]                  blk.5.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  54/ 363]                blk.5.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  55/ 363]                blk.5.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[  56/ 363]                blk.5.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  57/ 363]                  blk.5.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[  58/ 363]                  blk.6.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[  59/ 363]               blk.6.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  60/ 363]             blk.6.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  61/ 363]                  blk.6.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[  62/ 363]                  blk.6.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  63/ 363]                blk.6.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  64/ 363]                blk.6.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[  65/ 363]                blk.6.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  66/ 363]                  blk.6.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[  67/ 363]                  blk.7.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[  68/ 363]               blk.7.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  69/ 363]             blk.7.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  70/ 363]                  blk.7.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[  71/ 363]                  blk.7.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  72/ 363]                blk.7.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  73/ 363]                blk.7.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[  74/ 363]                blk.7.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  75/ 363]                  blk.7.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[  76/ 363]                  blk.8.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[  77/ 363]               blk.8.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  78/ 363]             blk.8.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  79/ 363]                  blk.8.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[  80/ 363]                  blk.8.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  81/ 363]                blk.8.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  82/ 363]                blk.8.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[  83/ 363]                blk.8.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  84/ 363]                  blk.8.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[  85/ 363]                  blk.9.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[  86/ 363]               blk.9.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  87/ 363]             blk.9.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  88/ 363]                  blk.9.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[  89/ 363]                  blk.9.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  90/ 363]                blk.9.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  91/ 363]                blk.9.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[  92/ 363]                blk.9.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  93/ 363]                  blk.9.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[  94/ 363]                 blk.10.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[  95/ 363]              blk.10.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  96/ 363]            blk.10.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  97/ 363]                 blk.10.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[  98/ 363]                 blk.10.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  99/ 363]               blk.10.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 100/ 363]               blk.10.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 101/ 363]               blk.10.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 102/ 363]                 blk.10.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 103/ 363]                 blk.11.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 104/ 363]              blk.11.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 105/ 363]            blk.11.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 106/ 363]                 blk.11.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 107/ 363]                 blk.11.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 108/ 363]               blk.11.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 109/ 363]               blk.11.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 110/ 363]               blk.11.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 111/ 363]                 blk.11.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 112/ 363]                 blk.12.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 113/ 363]              blk.12.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 114/ 363]            blk.12.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 115/ 363]                 blk.12.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 116/ 363]                 blk.12.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 117/ 363]               blk.12.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 118/ 363]               blk.12.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 119/ 363]               blk.12.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 120/ 363]                 blk.12.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 121/ 363]                 blk.13.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 122/ 363]              blk.13.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 123/ 363]            blk.13.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 124/ 363]                 blk.13.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 125/ 363]                 blk.13.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 126/ 363]               blk.13.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 127/ 363]               blk.13.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 128/ 363]               blk.13.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 129/ 363]                 blk.13.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 130/ 363]                 blk.14.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 131/ 363]              blk.14.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 132/ 363]            blk.14.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 133/ 363]                 blk.14.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 134/ 363]                 blk.14.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 135/ 363]               blk.14.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 136/ 363]               blk.14.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 137/ 363]               blk.14.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 138/ 363]                 blk.14.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 139/ 363]                 blk.15.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 140/ 363]              blk.15.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 141/ 363]            blk.15.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 142/ 363]                 blk.15.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 143/ 363]                 blk.15.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 144/ 363]               blk.15.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 145/ 363]               blk.15.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 146/ 363]               blk.15.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 147/ 363]                 blk.15.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 148/ 363]                 blk.16.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 149/ 363]              blk.16.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 150/ 363]            blk.16.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 151/ 363]                 blk.16.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 152/ 363]                 blk.16.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 153/ 363]               blk.16.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 154/ 363]               blk.16.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 155/ 363]               blk.16.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 156/ 363]                 blk.16.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 157/ 363]                 blk.17.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 158/ 363]              blk.17.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 159/ 363]            blk.17.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 160/ 363]                 blk.17.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 161/ 363]                 blk.17.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 162/ 363]               blk.17.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 163/ 363]               blk.17.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 164/ 363]               blk.17.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 165/ 363]                 blk.17.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 166/ 363]                 blk.18.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 167/ 363]              blk.18.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 168/ 363]            blk.18.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 169/ 363]                 blk.18.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 170/ 363]                 blk.18.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 171/ 363]               blk.18.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 172/ 363]               blk.18.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 173/ 363]               blk.18.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 174/ 363]                 blk.18.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 175/ 363]                 blk.19.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 176/ 363]              blk.19.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 177/ 363]            blk.19.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 178/ 363]                 blk.19.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 179/ 363]                 blk.19.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 180/ 363]               blk.19.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 181/ 363]               blk.19.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 182/ 363]               blk.19.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 183/ 363]                 blk.19.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 184/ 363]                 blk.20.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 185/ 363]              blk.20.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 186/ 363]            blk.20.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 187/ 363]                 blk.20.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 188/ 363]                 blk.20.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 189/ 363]               blk.20.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 190/ 363]               blk.20.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 191/ 363]               blk.20.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 192/ 363]                 blk.20.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 193/ 363]                 blk.21.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 194/ 363]              blk.21.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 195/ 363]            blk.21.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 196/ 363]                 blk.21.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 197/ 363]                 blk.21.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 198/ 363]               blk.21.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 199/ 363]               blk.21.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 200/ 363]               blk.21.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 201/ 363]                 blk.21.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 202/ 363]                 blk.22.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 203/ 363]              blk.22.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 204/ 363]            blk.22.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 205/ 363]                 blk.22.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 206/ 363]                 blk.22.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 207/ 363]               blk.22.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 208/ 363]               blk.22.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 209/ 363]               blk.22.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 210/ 363]                 blk.22.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 211/ 363]                 blk.23.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 212/ 363]              blk.23.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 213/ 363]            blk.23.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 214/ 363]                 blk.23.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 215/ 363]                 blk.23.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 216/ 363]               blk.23.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 217/ 363]               blk.23.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 218/ 363]               blk.23.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 219/ 363]                 blk.23.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 220/ 363]                 blk.24.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 221/ 363]              blk.24.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 222/ 363]            blk.24.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 223/ 363]                 blk.24.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 224/ 363]                 blk.24.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 225/ 363]               blk.24.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 226/ 363]               blk.24.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 227/ 363]               blk.24.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 228/ 363]                 blk.24.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 229/ 363]                 blk.25.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 230/ 363]              blk.25.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 231/ 363]            blk.25.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 232/ 363]                 blk.25.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 233/ 363]                 blk.25.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 234/ 363]               blk.25.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 235/ 363]               blk.25.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 236/ 363]               blk.25.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 237/ 363]                 blk.25.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 238/ 363]                 blk.26.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 239/ 363]              blk.26.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 240/ 363]            blk.26.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 241/ 363]                 blk.26.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 242/ 363]                 blk.26.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 243/ 363]               blk.26.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 244/ 363]               blk.26.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 245/ 363]               blk.26.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 246/ 363]                 blk.26.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 247/ 363]                 blk.27.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 248/ 363]              blk.27.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 249/ 363]            blk.27.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 250/ 363]                 blk.27.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 251/ 363]                 blk.27.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 252/ 363]               blk.27.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 253/ 363]               blk.27.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 254/ 363]               blk.27.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 255/ 363]                 blk.27.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 256/ 363]                 blk.28.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 257/ 363]              blk.28.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 258/ 363]            blk.28.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 259/ 363]                 blk.28.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 260/ 363]                 blk.28.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 261/ 363]               blk.28.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 262/ 363]               blk.28.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 263/ 363]               blk.28.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 264/ 363]                 blk.28.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 265/ 363]                 blk.29.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 266/ 363]              blk.29.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 267/ 363]            blk.29.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 268/ 363]                 blk.29.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 269/ 363]                 blk.29.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 270/ 363]               blk.29.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 271/ 363]               blk.29.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 272/ 363]               blk.29.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 273/ 363]                 blk.29.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 274/ 363]                 blk.30.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 275/ 363]              blk.30.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 276/ 363]            blk.30.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 277/ 363]                 blk.30.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 278/ 363]                 blk.30.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 279/ 363]               blk.30.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 280/ 363]               blk.30.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 281/ 363]               blk.30.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 282/ 363]                 blk.30.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 283/ 363]                 blk.31.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 284/ 363]              blk.31.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 285/ 363]            blk.31.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 286/ 363]                 blk.31.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 287/ 363]                 blk.31.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 288/ 363]               blk.31.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 289/ 363]               blk.31.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 290/ 363]               blk.31.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 291/ 363]                 blk.31.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 292/ 363]                 blk.32.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 293/ 363]              blk.32.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 294/ 363]            blk.32.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 295/ 363]                 blk.32.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 296/ 363]                 blk.32.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 297/ 363]               blk.32.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 298/ 363]               blk.32.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 299/ 363]               blk.32.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 300/ 363]                 blk.32.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 301/ 363]                 blk.33.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 302/ 363]              blk.33.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 303/ 363]            blk.33.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 304/ 363]                 blk.33.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 305/ 363]                 blk.33.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 306/ 363]               blk.33.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 307/ 363]               blk.33.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 308/ 363]               blk.33.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 309/ 363]                 blk.33.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 310/ 363]                 blk.34.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 311/ 363]              blk.34.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 312/ 363]            blk.34.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 313/ 363]                 blk.34.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 314/ 363]                 blk.34.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 315/ 363]               blk.34.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 316/ 363]               blk.34.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 317/ 363]               blk.34.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 318/ 363]                 blk.34.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 319/ 363]                 blk.35.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 320/ 363]              blk.35.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 321/ 363]            blk.35.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 322/ 363]                 blk.35.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 323/ 363]                 blk.35.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 324/ 363]               blk.35.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 325/ 363]               blk.35.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 326/ 363]               blk.35.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 327/ 363]                 blk.35.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 328/ 363]                 blk.36.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 329/ 363]              blk.36.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 330/ 363]            blk.36.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 331/ 363]                 blk.36.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 332/ 363]                 blk.36.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 333/ 363]               blk.36.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 334/ 363]               blk.36.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 335/ 363]               blk.36.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 336/ 363]                 blk.36.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 337/ 363]                 blk.37.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 338/ 363]              blk.37.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 339/ 363]            blk.37.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 340/ 363]                 blk.37.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 341/ 363]                 blk.37.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 342/ 363]               blk.37.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 343/ 363]               blk.37.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 344/ 363]               blk.37.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 345/ 363]                 blk.37.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 346/ 363]                 blk.38.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 347/ 363]              blk.38.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 348/ 363]            blk.38.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 349/ 363]                 blk.38.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 350/ 363]                 blk.38.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 351/ 363]               blk.38.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 352/ 363]               blk.38.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 353/ 363]               blk.38.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 354/ 363]                 blk.38.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 355/ 363]                 blk.39.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 356/ 363]              blk.39.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 357/ 363]            blk.39.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 358/ 363]                 blk.39.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q3_K .. size =    50.00 MiB ->    10.74 MiB\n",
            "[ 359/ 363]                 blk.39.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 360/ 363]               blk.39.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 361/ 363]               blk.39.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "[ 362/ 363]               blk.39.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 363/ 363]                 blk.39.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q3_K .. size =   135.00 MiB ->    29.00 MiB\n",
            "llama_model_quantize_impl: model size  = 24826.58 MB\n",
            "llama_model_quantize_impl: quant size  =  6043.46 MB\n",
            "\n",
            "main: quantize time = 368809.92 ms\n",
            "main:    total time = 368809.92 ms\n",
            "main: build = 5165 (1d735c0b)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/models/open_llama_13B/ggml-model-f16.gguf' to '/content/models/open_llama_13B/ggml-model-q4_k.gguf' as Q4_K\n",
            "llama_model_loader: loaded meta data with 24 key-value pairs and 363 tensors from /content/models/open_llama_13B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = open_llama_13B\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 13B\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
            "llama_model_loader: - kv   5:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 13824\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - type  f32:   81 tensors\n",
            "llama_model_loader: - type  f16:  282 tensors\n",
            "[   1/ 363]                        output.weight - [ 5120, 32000,     1,     1], type =    f16, converting to q6_K .. size =   312.50 MiB ->   128.17 MiB\n",
            "[   2/ 363]                   output_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[   3/ 363]                    token_embd.weight - [ 5120, 32000,     1,     1], type =    f16, converting to q4_K .. size =   312.50 MiB ->    87.89 MiB\n",
            "[   4/ 363]                  blk.0.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[   5/ 363]               blk.0.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[   6/ 363]             blk.0.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[   7/ 363]                  blk.0.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[   8/ 363]                  blk.0.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[   9/ 363]                blk.0.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[  10/ 363]                blk.0.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  11/ 363]                blk.0.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  12/ 363]                  blk.0.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  13/ 363]                  blk.1.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  14/ 363]               blk.1.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  15/ 363]             blk.1.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  16/ 363]                  blk.1.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  17/ 363]                  blk.1.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  18/ 363]                blk.1.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[  19/ 363]                blk.1.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  20/ 363]                blk.1.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  21/ 363]                  blk.1.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  22/ 363]                  blk.2.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  23/ 363]               blk.2.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  24/ 363]             blk.2.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  25/ 363]                  blk.2.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  26/ 363]                  blk.2.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  27/ 363]                blk.2.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[  28/ 363]                blk.2.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  29/ 363]                blk.2.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  30/ 363]                  blk.2.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  31/ 363]                  blk.3.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  32/ 363]               blk.3.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  33/ 363]             blk.3.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  34/ 363]                  blk.3.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  35/ 363]                  blk.3.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  36/ 363]                blk.3.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[  37/ 363]                blk.3.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  38/ 363]                blk.3.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  39/ 363]                  blk.3.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  40/ 363]                  blk.4.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  41/ 363]               blk.4.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  42/ 363]             blk.4.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  43/ 363]                  blk.4.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  44/ 363]                  blk.4.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  45/ 363]                blk.4.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[  46/ 363]                blk.4.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  47/ 363]                blk.4.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  48/ 363]                  blk.4.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  49/ 363]                  blk.5.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  50/ 363]               blk.5.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  51/ 363]             blk.5.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  52/ 363]                  blk.5.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  53/ 363]                  blk.5.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  54/ 363]                blk.5.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  55/ 363]                blk.5.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  56/ 363]                blk.5.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  57/ 363]                  blk.5.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  58/ 363]                  blk.6.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  59/ 363]               blk.6.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  60/ 363]             blk.6.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  61/ 363]                  blk.6.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  62/ 363]                  blk.6.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  63/ 363]                blk.6.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  64/ 363]                blk.6.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  65/ 363]                blk.6.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  66/ 363]                  blk.6.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  67/ 363]                  blk.7.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  68/ 363]               blk.7.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  69/ 363]             blk.7.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  70/ 363]                  blk.7.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  71/ 363]                  blk.7.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  72/ 363]                blk.7.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[  73/ 363]                blk.7.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  74/ 363]                blk.7.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  75/ 363]                  blk.7.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  76/ 363]                  blk.8.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  77/ 363]               blk.8.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  78/ 363]             blk.8.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  79/ 363]                  blk.8.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  80/ 363]                  blk.8.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  81/ 363]                blk.8.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  82/ 363]                blk.8.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  83/ 363]                blk.8.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  84/ 363]                  blk.8.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  85/ 363]                  blk.9.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  86/ 363]               blk.9.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  87/ 363]             blk.9.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  88/ 363]                  blk.9.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  89/ 363]                  blk.9.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  90/ 363]                blk.9.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  91/ 363]                blk.9.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  92/ 363]                blk.9.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  93/ 363]                  blk.9.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  94/ 363]                 blk.10.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  95/ 363]              blk.10.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  96/ 363]            blk.10.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  97/ 363]                 blk.10.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  98/ 363]                 blk.10.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  99/ 363]               blk.10.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 100/ 363]               blk.10.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 101/ 363]               blk.10.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 102/ 363]                 blk.10.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 103/ 363]                 blk.11.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 104/ 363]              blk.11.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 105/ 363]            blk.11.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 106/ 363]                 blk.11.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 107/ 363]                 blk.11.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 108/ 363]               blk.11.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 109/ 363]               blk.11.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 110/ 363]               blk.11.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 111/ 363]                 blk.11.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 112/ 363]                 blk.12.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 113/ 363]              blk.12.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 114/ 363]            blk.12.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 115/ 363]                 blk.12.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 116/ 363]                 blk.12.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 117/ 363]               blk.12.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 118/ 363]               blk.12.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 119/ 363]               blk.12.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 120/ 363]                 blk.12.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 121/ 363]                 blk.13.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 122/ 363]              blk.13.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 123/ 363]            blk.13.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 124/ 363]                 blk.13.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 125/ 363]                 blk.13.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 126/ 363]               blk.13.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 127/ 363]               blk.13.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 128/ 363]               blk.13.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 129/ 363]                 blk.13.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 130/ 363]                 blk.14.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 131/ 363]              blk.14.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 132/ 363]            blk.14.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 133/ 363]                 blk.14.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 134/ 363]                 blk.14.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 135/ 363]               blk.14.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 136/ 363]               blk.14.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 137/ 363]               blk.14.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 138/ 363]                 blk.14.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 139/ 363]                 blk.15.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 140/ 363]              blk.15.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 141/ 363]            blk.15.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 142/ 363]                 blk.15.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 143/ 363]                 blk.15.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 144/ 363]               blk.15.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 145/ 363]               blk.15.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 146/ 363]               blk.15.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 147/ 363]                 blk.15.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 148/ 363]                 blk.16.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 149/ 363]              blk.16.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 150/ 363]            blk.16.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 151/ 363]                 blk.16.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 152/ 363]                 blk.16.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 153/ 363]               blk.16.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 154/ 363]               blk.16.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 155/ 363]               blk.16.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 156/ 363]                 blk.16.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 157/ 363]                 blk.17.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 158/ 363]              blk.17.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 159/ 363]            blk.17.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 160/ 363]                 blk.17.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 161/ 363]                 blk.17.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 162/ 363]               blk.17.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 163/ 363]               blk.17.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 164/ 363]               blk.17.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 165/ 363]                 blk.17.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 166/ 363]                 blk.18.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 167/ 363]              blk.18.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 168/ 363]            blk.18.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 169/ 363]                 blk.18.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 170/ 363]                 blk.18.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 171/ 363]               blk.18.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 172/ 363]               blk.18.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 173/ 363]               blk.18.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 174/ 363]                 blk.18.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 175/ 363]                 blk.19.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 176/ 363]              blk.19.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 177/ 363]            blk.19.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 178/ 363]                 blk.19.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 179/ 363]                 blk.19.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 180/ 363]               blk.19.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 181/ 363]               blk.19.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 182/ 363]               blk.19.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 183/ 363]                 blk.19.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 184/ 363]                 blk.20.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 185/ 363]              blk.20.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 186/ 363]            blk.20.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 187/ 363]                 blk.20.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 188/ 363]                 blk.20.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 189/ 363]               blk.20.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 190/ 363]               blk.20.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 191/ 363]               blk.20.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 192/ 363]                 blk.20.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 193/ 363]                 blk.21.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 194/ 363]              blk.21.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 195/ 363]            blk.21.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 196/ 363]                 blk.21.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 197/ 363]                 blk.21.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 198/ 363]               blk.21.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 199/ 363]               blk.21.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 200/ 363]               blk.21.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 201/ 363]                 blk.21.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 202/ 363]                 blk.22.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 203/ 363]              blk.22.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 204/ 363]            blk.22.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 205/ 363]                 blk.22.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 206/ 363]                 blk.22.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 207/ 363]               blk.22.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 208/ 363]               blk.22.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 209/ 363]               blk.22.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 210/ 363]                 blk.22.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 211/ 363]                 blk.23.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 212/ 363]              blk.23.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 213/ 363]            blk.23.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 214/ 363]                 blk.23.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 215/ 363]                 blk.23.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 216/ 363]               blk.23.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 217/ 363]               blk.23.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 218/ 363]               blk.23.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 219/ 363]                 blk.23.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 220/ 363]                 blk.24.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 221/ 363]              blk.24.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 222/ 363]            blk.24.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 223/ 363]                 blk.24.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 224/ 363]                 blk.24.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 225/ 363]               blk.24.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 226/ 363]               blk.24.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 227/ 363]               blk.24.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 228/ 363]                 blk.24.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 229/ 363]                 blk.25.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 230/ 363]              blk.25.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 231/ 363]            blk.25.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 232/ 363]                 blk.25.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 233/ 363]                 blk.25.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 234/ 363]               blk.25.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 235/ 363]               blk.25.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 236/ 363]               blk.25.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 237/ 363]                 blk.25.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 238/ 363]                 blk.26.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 239/ 363]              blk.26.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 240/ 363]            blk.26.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 241/ 363]                 blk.26.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 242/ 363]                 blk.26.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 243/ 363]               blk.26.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 244/ 363]               blk.26.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 245/ 363]               blk.26.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 246/ 363]                 blk.26.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 247/ 363]                 blk.27.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 248/ 363]              blk.27.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 249/ 363]            blk.27.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 250/ 363]                 blk.27.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 251/ 363]                 blk.27.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 252/ 363]               blk.27.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 253/ 363]               blk.27.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 254/ 363]               blk.27.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 255/ 363]                 blk.27.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 256/ 363]                 blk.28.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 257/ 363]              blk.28.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 258/ 363]            blk.28.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 259/ 363]                 blk.28.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 260/ 363]                 blk.28.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 261/ 363]               blk.28.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 262/ 363]               blk.28.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 263/ 363]               blk.28.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 264/ 363]                 blk.28.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 265/ 363]                 blk.29.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 266/ 363]              blk.29.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 267/ 363]            blk.29.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 268/ 363]                 blk.29.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 269/ 363]                 blk.29.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 270/ 363]               blk.29.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 271/ 363]               blk.29.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 272/ 363]               blk.29.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 273/ 363]                 blk.29.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 274/ 363]                 blk.30.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 275/ 363]              blk.30.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 276/ 363]            blk.30.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 277/ 363]                 blk.30.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 278/ 363]                 blk.30.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 279/ 363]               blk.30.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 280/ 363]               blk.30.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 281/ 363]               blk.30.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 282/ 363]                 blk.30.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 283/ 363]                 blk.31.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 284/ 363]              blk.31.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 285/ 363]            blk.31.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 286/ 363]                 blk.31.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 287/ 363]                 blk.31.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 288/ 363]               blk.31.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 289/ 363]               blk.31.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 290/ 363]               blk.31.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 291/ 363]                 blk.31.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 292/ 363]                 blk.32.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 293/ 363]              blk.32.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 294/ 363]            blk.32.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 295/ 363]                 blk.32.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 296/ 363]                 blk.32.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 297/ 363]               blk.32.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 298/ 363]               blk.32.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 299/ 363]               blk.32.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 300/ 363]                 blk.32.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 301/ 363]                 blk.33.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 302/ 363]              blk.33.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 303/ 363]            blk.33.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 304/ 363]                 blk.33.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 305/ 363]                 blk.33.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 306/ 363]               blk.33.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 307/ 363]               blk.33.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 308/ 363]               blk.33.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 309/ 363]                 blk.33.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 310/ 363]                 blk.34.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 311/ 363]              blk.34.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 312/ 363]            blk.34.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 313/ 363]                 blk.34.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 314/ 363]                 blk.34.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 315/ 363]               blk.34.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 316/ 363]               blk.34.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 317/ 363]               blk.34.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 318/ 363]                 blk.34.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 319/ 363]                 blk.35.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 320/ 363]              blk.35.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 321/ 363]            blk.35.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 322/ 363]                 blk.35.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 323/ 363]                 blk.35.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 324/ 363]               blk.35.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 325/ 363]               blk.35.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 326/ 363]               blk.35.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 327/ 363]                 blk.35.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 328/ 363]                 blk.36.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 329/ 363]              blk.36.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 330/ 363]            blk.36.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 331/ 363]                 blk.36.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 332/ 363]                 blk.36.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 333/ 363]               blk.36.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 334/ 363]               blk.36.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 335/ 363]               blk.36.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 336/ 363]                 blk.36.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 337/ 363]                 blk.37.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 338/ 363]              blk.37.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 339/ 363]            blk.37.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 340/ 363]                 blk.37.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 341/ 363]                 blk.37.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 342/ 363]               blk.37.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 343/ 363]               blk.37.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 344/ 363]               blk.37.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 345/ 363]                 blk.37.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 346/ 363]                 blk.38.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 347/ 363]              blk.38.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 348/ 363]            blk.38.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 349/ 363]                 blk.38.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 350/ 363]                 blk.38.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 351/ 363]               blk.38.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 352/ 363]               blk.38.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 353/ 363]               blk.38.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 354/ 363]                 blk.38.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 355/ 363]                 blk.39.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 356/ 363]              blk.39.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 357/ 363]            blk.39.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 358/ 363]                 blk.39.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 359/ 363]                 blk.39.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 360/ 363]               blk.39.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 361/ 363]               blk.39.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 362/ 363]               blk.39.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 363/ 363]                 blk.39.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "llama_model_quantize_impl: model size  = 24826.58 MB\n",
            "llama_model_quantize_impl: quant size  =  7500.85 MB\n",
            "\n",
            "main: quantize time = 630930.35 ms\n",
            "main:    total time = 630930.35 ms\n",
            "main: build = 5165 (1d735c0b)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/models/open_llama_13B/ggml-model-f16.gguf' to '/content/models/open_llama_13B/ggml-model-q5_k.gguf' as Q5_K\n",
            "llama_model_loader: loaded meta data with 24 key-value pairs and 363 tensors from /content/models/open_llama_13B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = open_llama_13B\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 13B\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
            "llama_model_loader: - kv   5:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 13824\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - type  f32:   81 tensors\n",
            "llama_model_loader: - type  f16:  282 tensors\n",
            "[   1/ 363]                        output.weight - [ 5120, 32000,     1,     1], type =    f16, converting to q6_K .. size =   312.50 MiB ->   128.17 MiB\n",
            "[   2/ 363]                   output_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[   3/ 363]                    token_embd.weight - [ 5120, 32000,     1,     1], type =    f16, converting to q5_K .. size =   312.50 MiB ->   107.42 MiB\n",
            "[   4/ 363]                  blk.0.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[   5/ 363]               blk.0.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[   6/ 363]             blk.0.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[   7/ 363]                  blk.0.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[   8/ 363]                  blk.0.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[   9/ 363]                blk.0.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[  10/ 363]                blk.0.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  11/ 363]                blk.0.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  12/ 363]                  blk.0.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  13/ 363]                  blk.1.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  14/ 363]               blk.1.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  15/ 363]             blk.1.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  16/ 363]                  blk.1.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  17/ 363]                  blk.1.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  18/ 363]                blk.1.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[  19/ 363]                blk.1.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  20/ 363]                blk.1.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  21/ 363]                  blk.1.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  22/ 363]                  blk.2.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  23/ 363]               blk.2.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  24/ 363]             blk.2.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  25/ 363]                  blk.2.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  26/ 363]                  blk.2.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  27/ 363]                blk.2.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[  28/ 363]                blk.2.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  29/ 363]                blk.2.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  30/ 363]                  blk.2.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  31/ 363]                  blk.3.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  32/ 363]               blk.3.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  33/ 363]             blk.3.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  34/ 363]                  blk.3.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  35/ 363]                  blk.3.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  36/ 363]                blk.3.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[  37/ 363]                blk.3.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  38/ 363]                blk.3.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  39/ 363]                  blk.3.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  40/ 363]                  blk.4.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  41/ 363]               blk.4.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  42/ 363]             blk.4.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  43/ 363]                  blk.4.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  44/ 363]                  blk.4.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  45/ 363]                blk.4.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[  46/ 363]                blk.4.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  47/ 363]                blk.4.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  48/ 363]                  blk.4.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  49/ 363]                  blk.5.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  50/ 363]               blk.5.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  51/ 363]             blk.5.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  52/ 363]                  blk.5.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  53/ 363]                  blk.5.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  54/ 363]                blk.5.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  55/ 363]                blk.5.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  56/ 363]                blk.5.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  57/ 363]                  blk.5.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  58/ 363]                  blk.6.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  59/ 363]               blk.6.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  60/ 363]             blk.6.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  61/ 363]                  blk.6.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  62/ 363]                  blk.6.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  63/ 363]                blk.6.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  64/ 363]                blk.6.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  65/ 363]                blk.6.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  66/ 363]                  blk.6.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  67/ 363]                  blk.7.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  68/ 363]               blk.7.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  69/ 363]             blk.7.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  70/ 363]                  blk.7.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  71/ 363]                  blk.7.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  72/ 363]                blk.7.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[  73/ 363]                blk.7.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  74/ 363]                blk.7.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  75/ 363]                  blk.7.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  76/ 363]                  blk.8.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  77/ 363]               blk.8.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  78/ 363]             blk.8.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  79/ 363]                  blk.8.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  80/ 363]                  blk.8.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  81/ 363]                blk.8.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  82/ 363]                blk.8.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  83/ 363]                blk.8.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  84/ 363]                  blk.8.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  85/ 363]                  blk.9.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  86/ 363]               blk.9.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  87/ 363]             blk.9.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  88/ 363]                  blk.9.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  89/ 363]                  blk.9.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  90/ 363]                blk.9.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  91/ 363]                blk.9.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  92/ 363]                blk.9.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  93/ 363]                  blk.9.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[  94/ 363]                 blk.10.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  95/ 363]              blk.10.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  96/ 363]            blk.10.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  97/ 363]                 blk.10.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  98/ 363]                 blk.10.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  99/ 363]               blk.10.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 100/ 363]               blk.10.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 101/ 363]               blk.10.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 102/ 363]                 blk.10.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 103/ 363]                 blk.11.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 104/ 363]              blk.11.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 105/ 363]            blk.11.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 106/ 363]                 blk.11.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 107/ 363]                 blk.11.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 108/ 363]               blk.11.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 109/ 363]               blk.11.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 110/ 363]               blk.11.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 111/ 363]                 blk.11.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 112/ 363]                 blk.12.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 113/ 363]              blk.12.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 114/ 363]            blk.12.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 115/ 363]                 blk.12.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 116/ 363]                 blk.12.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 117/ 363]               blk.12.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 118/ 363]               blk.12.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 119/ 363]               blk.12.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 120/ 363]                 blk.12.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 121/ 363]                 blk.13.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 122/ 363]              blk.13.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 123/ 363]            blk.13.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 124/ 363]                 blk.13.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 125/ 363]                 blk.13.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 126/ 363]               blk.13.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 127/ 363]               blk.13.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 128/ 363]               blk.13.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 129/ 363]                 blk.13.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 130/ 363]                 blk.14.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 131/ 363]              blk.14.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 132/ 363]            blk.14.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 133/ 363]                 blk.14.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 134/ 363]                 blk.14.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 135/ 363]               blk.14.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 136/ 363]               blk.14.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 137/ 363]               blk.14.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 138/ 363]                 blk.14.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 139/ 363]                 blk.15.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 140/ 363]              blk.15.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 141/ 363]            blk.15.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 142/ 363]                 blk.15.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 143/ 363]                 blk.15.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 144/ 363]               blk.15.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 145/ 363]               blk.15.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 146/ 363]               blk.15.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 147/ 363]                 blk.15.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 148/ 363]                 blk.16.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 149/ 363]              blk.16.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 150/ 363]            blk.16.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 151/ 363]                 blk.16.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 152/ 363]                 blk.16.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 153/ 363]               blk.16.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 154/ 363]               blk.16.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 155/ 363]               blk.16.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 156/ 363]                 blk.16.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 157/ 363]                 blk.17.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 158/ 363]              blk.17.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 159/ 363]            blk.17.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 160/ 363]                 blk.17.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 161/ 363]                 blk.17.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 162/ 363]               blk.17.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 163/ 363]               blk.17.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 164/ 363]               blk.17.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 165/ 363]                 blk.17.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 166/ 363]                 blk.18.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 167/ 363]              blk.18.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 168/ 363]            blk.18.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 169/ 363]                 blk.18.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 170/ 363]                 blk.18.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 171/ 363]               blk.18.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 172/ 363]               blk.18.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 173/ 363]               blk.18.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 174/ 363]                 blk.18.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 175/ 363]                 blk.19.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 176/ 363]              blk.19.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 177/ 363]            blk.19.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 178/ 363]                 blk.19.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 179/ 363]                 blk.19.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 180/ 363]               blk.19.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 181/ 363]               blk.19.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 182/ 363]               blk.19.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 183/ 363]                 blk.19.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 184/ 363]                 blk.20.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 185/ 363]              blk.20.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 186/ 363]            blk.20.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 187/ 363]                 blk.20.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 188/ 363]                 blk.20.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 189/ 363]               blk.20.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 190/ 363]               blk.20.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 191/ 363]               blk.20.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 192/ 363]                 blk.20.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 193/ 363]                 blk.21.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 194/ 363]              blk.21.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 195/ 363]            blk.21.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 196/ 363]                 blk.21.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 197/ 363]                 blk.21.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 198/ 363]               blk.21.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 199/ 363]               blk.21.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 200/ 363]               blk.21.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 201/ 363]                 blk.21.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 202/ 363]                 blk.22.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 203/ 363]              blk.22.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 204/ 363]            blk.22.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 205/ 363]                 blk.22.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 206/ 363]                 blk.22.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 207/ 363]               blk.22.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 208/ 363]               blk.22.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 209/ 363]               blk.22.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 210/ 363]                 blk.22.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 211/ 363]                 blk.23.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 212/ 363]              blk.23.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 213/ 363]            blk.23.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 214/ 363]                 blk.23.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 215/ 363]                 blk.23.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 216/ 363]               blk.23.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 217/ 363]               blk.23.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 218/ 363]               blk.23.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 219/ 363]                 blk.23.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 220/ 363]                 blk.24.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 221/ 363]              blk.24.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 222/ 363]            blk.24.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 223/ 363]                 blk.24.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 224/ 363]                 blk.24.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 225/ 363]               blk.24.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 226/ 363]               blk.24.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 227/ 363]               blk.24.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 228/ 363]                 blk.24.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 229/ 363]                 blk.25.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 230/ 363]              blk.25.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 231/ 363]            blk.25.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 232/ 363]                 blk.25.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 233/ 363]                 blk.25.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 234/ 363]               blk.25.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 235/ 363]               blk.25.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 236/ 363]               blk.25.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 237/ 363]                 blk.25.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 238/ 363]                 blk.26.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 239/ 363]              blk.26.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 240/ 363]            blk.26.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 241/ 363]                 blk.26.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 242/ 363]                 blk.26.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 243/ 363]               blk.26.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 244/ 363]               blk.26.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 245/ 363]               blk.26.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 246/ 363]                 blk.26.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 247/ 363]                 blk.27.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 248/ 363]              blk.27.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 249/ 363]            blk.27.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 250/ 363]                 blk.27.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 251/ 363]                 blk.27.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 252/ 363]               blk.27.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 253/ 363]               blk.27.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 254/ 363]               blk.27.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 255/ 363]                 blk.27.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 256/ 363]                 blk.28.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 257/ 363]              blk.28.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 258/ 363]            blk.28.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 259/ 363]                 blk.28.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 260/ 363]                 blk.28.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 261/ 363]               blk.28.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 262/ 363]               blk.28.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 263/ 363]               blk.28.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 264/ 363]                 blk.28.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 265/ 363]                 blk.29.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 266/ 363]              blk.29.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 267/ 363]            blk.29.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 268/ 363]                 blk.29.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 269/ 363]                 blk.29.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 270/ 363]               blk.29.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 271/ 363]               blk.29.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 272/ 363]               blk.29.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 273/ 363]                 blk.29.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 274/ 363]                 blk.30.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 275/ 363]              blk.30.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 276/ 363]            blk.30.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 277/ 363]                 blk.30.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 278/ 363]                 blk.30.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 279/ 363]               blk.30.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 280/ 363]               blk.30.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 281/ 363]               blk.30.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 282/ 363]                 blk.30.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 283/ 363]                 blk.31.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 284/ 363]              blk.31.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 285/ 363]            blk.31.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 286/ 363]                 blk.31.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 287/ 363]                 blk.31.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 288/ 363]               blk.31.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 289/ 363]               blk.31.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 290/ 363]               blk.31.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 291/ 363]                 blk.31.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 292/ 363]                 blk.32.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 293/ 363]              blk.32.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 294/ 363]            blk.32.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 295/ 363]                 blk.32.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 296/ 363]                 blk.32.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 297/ 363]               blk.32.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 298/ 363]               blk.32.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 299/ 363]               blk.32.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 300/ 363]                 blk.32.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 301/ 363]                 blk.33.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 302/ 363]              blk.33.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 303/ 363]            blk.33.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 304/ 363]                 blk.33.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 305/ 363]                 blk.33.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 306/ 363]               blk.33.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 307/ 363]               blk.33.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 308/ 363]               blk.33.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 309/ 363]                 blk.33.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 310/ 363]                 blk.34.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 311/ 363]              blk.34.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 312/ 363]            blk.34.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 313/ 363]                 blk.34.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 314/ 363]                 blk.34.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 315/ 363]               blk.34.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 316/ 363]               blk.34.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 317/ 363]               blk.34.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 318/ 363]                 blk.34.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 319/ 363]                 blk.35.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 320/ 363]              blk.35.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 321/ 363]            blk.35.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 322/ 363]                 blk.35.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 323/ 363]                 blk.35.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 324/ 363]               blk.35.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 325/ 363]               blk.35.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 326/ 363]               blk.35.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 327/ 363]                 blk.35.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 328/ 363]                 blk.36.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 329/ 363]              blk.36.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 330/ 363]            blk.36.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 331/ 363]                 blk.36.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 332/ 363]                 blk.36.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 333/ 363]               blk.36.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 334/ 363]               blk.36.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 335/ 363]               blk.36.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 336/ 363]                 blk.36.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 337/ 363]                 blk.37.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 338/ 363]              blk.37.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 339/ 363]            blk.37.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 340/ 363]                 blk.37.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 341/ 363]                 blk.37.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 342/ 363]               blk.37.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 343/ 363]               blk.37.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 344/ 363]               blk.37.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 345/ 363]                 blk.37.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 346/ 363]                 blk.38.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 347/ 363]              blk.38.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 348/ 363]            blk.38.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 349/ 363]                 blk.38.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 350/ 363]                 blk.38.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 351/ 363]               blk.38.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 352/ 363]               blk.38.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 353/ 363]               blk.38.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 354/ 363]                 blk.38.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 355/ 363]                 blk.39.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 356/ 363]              blk.39.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 357/ 363]            blk.39.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 358/ 363]                 blk.39.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 359/ 363]                 blk.39.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 360/ 363]               blk.39.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 361/ 363]               blk.39.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "[ 362/ 363]               blk.39.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 363/ 363]                 blk.39.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q5_K .. size =   135.00 MiB ->    46.41 MiB\n",
            "llama_model_quantize_impl: model size  = 24826.58 MB\n",
            "llama_model_quantize_impl: quant size  =  8801.63 MB\n",
            "\n",
            "main: quantize time = 527803.91 ms\n",
            "main:    total time = 527803.91 ms\n",
            "main: build = 5165 (1d735c0b)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/models/open_llama_13B/ggml-model-f16.gguf' to '/content/models/open_llama_13B/ggml-model-q8_0.gguf' as Q8_0\n",
            "llama_model_loader: loaded meta data with 24 key-value pairs and 363 tensors from /content/models/open_llama_13B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = open_llama_13B\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 13B\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
            "llama_model_loader: - kv   5:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 13824\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - type  f32:   81 tensors\n",
            "llama_model_loader: - type  f16:  282 tensors\n",
            "[   1/ 363]                        output.weight - [ 5120, 32000,     1,     1], type =    f16, converting to q8_0 .. size =   312.50 MiB ->   166.02 MiB\n",
            "[   2/ 363]                   output_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[   3/ 363]                    token_embd.weight - [ 5120, 32000,     1,     1], type =    f16, converting to q8_0 .. size =   312.50 MiB ->   166.02 MiB\n",
            "[   4/ 363]                  blk.0.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[   5/ 363]               blk.0.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[   6/ 363]             blk.0.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[   7/ 363]                  blk.0.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[   8/ 363]                  blk.0.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[   9/ 363]                blk.0.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  10/ 363]                blk.0.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  11/ 363]                blk.0.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  12/ 363]                  blk.0.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  13/ 363]                  blk.1.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  14/ 363]               blk.1.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  15/ 363]             blk.1.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  16/ 363]                  blk.1.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  17/ 363]                  blk.1.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  18/ 363]                blk.1.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  19/ 363]                blk.1.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  20/ 363]                blk.1.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  21/ 363]                  blk.1.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  22/ 363]                  blk.2.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  23/ 363]               blk.2.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  24/ 363]             blk.2.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  25/ 363]                  blk.2.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  26/ 363]                  blk.2.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  27/ 363]                blk.2.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  28/ 363]                blk.2.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  29/ 363]                blk.2.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  30/ 363]                  blk.2.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  31/ 363]                  blk.3.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  32/ 363]               blk.3.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  33/ 363]             blk.3.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  34/ 363]                  blk.3.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  35/ 363]                  blk.3.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  36/ 363]                blk.3.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  37/ 363]                blk.3.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  38/ 363]                blk.3.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  39/ 363]                  blk.3.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  40/ 363]                  blk.4.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  41/ 363]               blk.4.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  42/ 363]             blk.4.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  43/ 363]                  blk.4.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  44/ 363]                  blk.4.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  45/ 363]                blk.4.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  46/ 363]                blk.4.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  47/ 363]                blk.4.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  48/ 363]                  blk.4.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  49/ 363]                  blk.5.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  50/ 363]               blk.5.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  51/ 363]             blk.5.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  52/ 363]                  blk.5.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  53/ 363]                  blk.5.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  54/ 363]                blk.5.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  55/ 363]                blk.5.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  56/ 363]                blk.5.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  57/ 363]                  blk.5.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  58/ 363]                  blk.6.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  59/ 363]               blk.6.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  60/ 363]             blk.6.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  61/ 363]                  blk.6.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  62/ 363]                  blk.6.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  63/ 363]                blk.6.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  64/ 363]                blk.6.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  65/ 363]                blk.6.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  66/ 363]                  blk.6.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  67/ 363]                  blk.7.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  68/ 363]               blk.7.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  69/ 363]             blk.7.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  70/ 363]                  blk.7.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  71/ 363]                  blk.7.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  72/ 363]                blk.7.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  73/ 363]                blk.7.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  74/ 363]                blk.7.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  75/ 363]                  blk.7.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  76/ 363]                  blk.8.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  77/ 363]               blk.8.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  78/ 363]             blk.8.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  79/ 363]                  blk.8.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  80/ 363]                  blk.8.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  81/ 363]                blk.8.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  82/ 363]                blk.8.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  83/ 363]                blk.8.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  84/ 363]                  blk.8.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  85/ 363]                  blk.9.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  86/ 363]               blk.9.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  87/ 363]             blk.9.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  88/ 363]                  blk.9.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  89/ 363]                  blk.9.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  90/ 363]                blk.9.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  91/ 363]                blk.9.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  92/ 363]                blk.9.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  93/ 363]                  blk.9.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  94/ 363]                 blk.10.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  95/ 363]              blk.10.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  96/ 363]            blk.10.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  97/ 363]                 blk.10.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  98/ 363]                 blk.10.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  99/ 363]               blk.10.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 100/ 363]               blk.10.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 101/ 363]               blk.10.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 102/ 363]                 blk.10.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 103/ 363]                 blk.11.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 104/ 363]              blk.11.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 105/ 363]            blk.11.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 106/ 363]                 blk.11.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 107/ 363]                 blk.11.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 108/ 363]               blk.11.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 109/ 363]               blk.11.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 110/ 363]               blk.11.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 111/ 363]                 blk.11.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 112/ 363]                 blk.12.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 113/ 363]              blk.12.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 114/ 363]            blk.12.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 115/ 363]                 blk.12.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 116/ 363]                 blk.12.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 117/ 363]               blk.12.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 118/ 363]               blk.12.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 119/ 363]               blk.12.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 120/ 363]                 blk.12.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 121/ 363]                 blk.13.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 122/ 363]              blk.13.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 123/ 363]            blk.13.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 124/ 363]                 blk.13.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 125/ 363]                 blk.13.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 126/ 363]               blk.13.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 127/ 363]               blk.13.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 128/ 363]               blk.13.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 129/ 363]                 blk.13.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 130/ 363]                 blk.14.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 131/ 363]              blk.14.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 132/ 363]            blk.14.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 133/ 363]                 blk.14.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 134/ 363]                 blk.14.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 135/ 363]               blk.14.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 136/ 363]               blk.14.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 137/ 363]               blk.14.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 138/ 363]                 blk.14.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 139/ 363]                 blk.15.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 140/ 363]              blk.15.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 141/ 363]            blk.15.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 142/ 363]                 blk.15.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 143/ 363]                 blk.15.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 144/ 363]               blk.15.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 145/ 363]               blk.15.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 146/ 363]               blk.15.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 147/ 363]                 blk.15.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 148/ 363]                 blk.16.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 149/ 363]              blk.16.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 150/ 363]            blk.16.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 151/ 363]                 blk.16.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 152/ 363]                 blk.16.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 153/ 363]               blk.16.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 154/ 363]               blk.16.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 155/ 363]               blk.16.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 156/ 363]                 blk.16.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 157/ 363]                 blk.17.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 158/ 363]              blk.17.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 159/ 363]            blk.17.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 160/ 363]                 blk.17.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 161/ 363]                 blk.17.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 162/ 363]               blk.17.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 163/ 363]               blk.17.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 164/ 363]               blk.17.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 165/ 363]                 blk.17.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 166/ 363]                 blk.18.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 167/ 363]              blk.18.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 168/ 363]            blk.18.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 169/ 363]                 blk.18.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 170/ 363]                 blk.18.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 171/ 363]               blk.18.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 172/ 363]               blk.18.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 173/ 363]               blk.18.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 174/ 363]                 blk.18.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 175/ 363]                 blk.19.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 176/ 363]              blk.19.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 177/ 363]            blk.19.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 178/ 363]                 blk.19.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 179/ 363]                 blk.19.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 180/ 363]               blk.19.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 181/ 363]               blk.19.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 182/ 363]               blk.19.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 183/ 363]                 blk.19.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 184/ 363]                 blk.20.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 185/ 363]              blk.20.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 186/ 363]            blk.20.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 187/ 363]                 blk.20.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 188/ 363]                 blk.20.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 189/ 363]               blk.20.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 190/ 363]               blk.20.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 191/ 363]               blk.20.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 192/ 363]                 blk.20.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 193/ 363]                 blk.21.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 194/ 363]              blk.21.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 195/ 363]            blk.21.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 196/ 363]                 blk.21.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 197/ 363]                 blk.21.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 198/ 363]               blk.21.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 199/ 363]               blk.21.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 200/ 363]               blk.21.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 201/ 363]                 blk.21.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 202/ 363]                 blk.22.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 203/ 363]              blk.22.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 204/ 363]            blk.22.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 205/ 363]                 blk.22.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 206/ 363]                 blk.22.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 207/ 363]               blk.22.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 208/ 363]               blk.22.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 209/ 363]               blk.22.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 210/ 363]                 blk.22.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 211/ 363]                 blk.23.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 212/ 363]              blk.23.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 213/ 363]            blk.23.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 214/ 363]                 blk.23.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 215/ 363]                 blk.23.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 216/ 363]               blk.23.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 217/ 363]               blk.23.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 218/ 363]               blk.23.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 219/ 363]                 blk.23.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 220/ 363]                 blk.24.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 221/ 363]              blk.24.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 222/ 363]            blk.24.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 223/ 363]                 blk.24.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 224/ 363]                 blk.24.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 225/ 363]               blk.24.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 226/ 363]               blk.24.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 227/ 363]               blk.24.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 228/ 363]                 blk.24.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 229/ 363]                 blk.25.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 230/ 363]              blk.25.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 231/ 363]            blk.25.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 232/ 363]                 blk.25.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 233/ 363]                 blk.25.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 234/ 363]               blk.25.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 235/ 363]               blk.25.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 236/ 363]               blk.25.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 237/ 363]                 blk.25.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 238/ 363]                 blk.26.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 239/ 363]              blk.26.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 240/ 363]            blk.26.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 241/ 363]                 blk.26.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 242/ 363]                 blk.26.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 243/ 363]               blk.26.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 244/ 363]               blk.26.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 245/ 363]               blk.26.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 246/ 363]                 blk.26.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 247/ 363]                 blk.27.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 248/ 363]              blk.27.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 249/ 363]            blk.27.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 250/ 363]                 blk.27.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 251/ 363]                 blk.27.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 252/ 363]               blk.27.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 253/ 363]               blk.27.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 254/ 363]               blk.27.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 255/ 363]                 blk.27.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 256/ 363]                 blk.28.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 257/ 363]              blk.28.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 258/ 363]            blk.28.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 259/ 363]                 blk.28.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 260/ 363]                 blk.28.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 261/ 363]               blk.28.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 262/ 363]               blk.28.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 263/ 363]               blk.28.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 264/ 363]                 blk.28.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 265/ 363]                 blk.29.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 266/ 363]              blk.29.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 267/ 363]            blk.29.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 268/ 363]                 blk.29.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 269/ 363]                 blk.29.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 270/ 363]               blk.29.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 271/ 363]               blk.29.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 272/ 363]               blk.29.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 273/ 363]                 blk.29.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 274/ 363]                 blk.30.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 275/ 363]              blk.30.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 276/ 363]            blk.30.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 277/ 363]                 blk.30.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 278/ 363]                 blk.30.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 279/ 363]               blk.30.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 280/ 363]               blk.30.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 281/ 363]               blk.30.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 282/ 363]                 blk.30.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 283/ 363]                 blk.31.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 284/ 363]              blk.31.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 285/ 363]            blk.31.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 286/ 363]                 blk.31.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 287/ 363]                 blk.31.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 288/ 363]               blk.31.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 289/ 363]               blk.31.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 290/ 363]               blk.31.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 291/ 363]                 blk.31.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 292/ 363]                 blk.32.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 293/ 363]              blk.32.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 294/ 363]            blk.32.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 295/ 363]                 blk.32.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 296/ 363]                 blk.32.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 297/ 363]               blk.32.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 298/ 363]               blk.32.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 299/ 363]               blk.32.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 300/ 363]                 blk.32.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 301/ 363]                 blk.33.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 302/ 363]              blk.33.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 303/ 363]            blk.33.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 304/ 363]                 blk.33.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 305/ 363]                 blk.33.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 306/ 363]               blk.33.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 307/ 363]               blk.33.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 308/ 363]               blk.33.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 309/ 363]                 blk.33.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 310/ 363]                 blk.34.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 311/ 363]              blk.34.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 312/ 363]            blk.34.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 313/ 363]                 blk.34.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 314/ 363]                 blk.34.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 315/ 363]               blk.34.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 316/ 363]               blk.34.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 317/ 363]               blk.34.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 318/ 363]                 blk.34.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 319/ 363]                 blk.35.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 320/ 363]              blk.35.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 321/ 363]            blk.35.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 322/ 363]                 blk.35.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 323/ 363]                 blk.35.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 324/ 363]               blk.35.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 325/ 363]               blk.35.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 326/ 363]               blk.35.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 327/ 363]                 blk.35.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 328/ 363]                 blk.36.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 329/ 363]              blk.36.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 330/ 363]            blk.36.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 331/ 363]                 blk.36.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 332/ 363]                 blk.36.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 333/ 363]               blk.36.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 334/ 363]               blk.36.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 335/ 363]               blk.36.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 336/ 363]                 blk.36.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 337/ 363]                 blk.37.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 338/ 363]              blk.37.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 339/ 363]            blk.37.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 340/ 363]                 blk.37.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 341/ 363]                 blk.37.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 342/ 363]               blk.37.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 343/ 363]               blk.37.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 344/ 363]               blk.37.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 345/ 363]                 blk.37.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 346/ 363]                 blk.38.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 347/ 363]              blk.38.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 348/ 363]            blk.38.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 349/ 363]                 blk.38.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 350/ 363]                 blk.38.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 351/ 363]               blk.38.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 352/ 363]               blk.38.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 353/ 363]               blk.38.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 354/ 363]                 blk.38.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 355/ 363]                 blk.39.attn_k.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 356/ 363]              blk.39.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 357/ 363]            blk.39.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 358/ 363]                 blk.39.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 359/ 363]                 blk.39.attn_v.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 360/ 363]               blk.39.ffn_down.weight - [13824,  5120,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 361/ 363]               blk.39.ffn_gate.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 362/ 363]               blk.39.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 363/ 363]                 blk.39.ffn_up.weight - [ 5120, 13824,     1,     1], type =    f16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "llama_model_quantize_impl: model size  = 24826.58 MB\n",
            "llama_model_quantize_impl: quant size  = 13189.86 MB\n",
            "\n",
            "main: quantize time = 107557.27 ms\n",
            "main:    total time = 107557.27 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 5. Измерение перплексии\n",
        "\n",
        "# %%\n",
        "results = []\n",
        "\n",
        "def run_perplexity_test(model_file, quant_type, ctx_length):\n",
        "    try:\n",
        "\n",
        "        output = !./llama.cpp/build/bin/llama-perplexity -m {model_file} -f \"/content/llama.cpp/prompts/mnemonics.txt\" -c {ctx_length}\n",
        "\n",
        "\n",
        "        ppl = None\n",
        "\n",
        "        for elem in output:\n",
        "            if \"PPL\" in elem:\n",
        "                ppl = elem.split(\" = \")[-1]\n",
        "                print(ppl)\n",
        "                break\n",
        "\n",
        "        if ppl is not None:\n",
        "            size_gb = os.path.getsize(model_file)/(1024**3)\n",
        "            return ppl, size_gb\n",
        "\n",
        "    except:# error as e:# subprocess.CalledProcessError as e:\n",
        "        print(f\"Ошибка для {quant_type} (ctx {ctx_length})\")\n",
        "\n",
        "        return None, None\n",
        "\n",
        "# Запуск тестов для всех вариантов\n",
        "for quant in QUANT_TYPES + [\"f16\"]: #[\"f16\"] +\n",
        "    model_file = f\"/content/models/open_llama_{MODEL_SIZE}/ggml-model-{quant}.gguf\"\n",
        "\n",
        "    if not os.path.exists(model_file):\n",
        "        continue\n",
        "\n",
        "    for ctx in CONTEXT_LENGTHS:\n",
        "        print(f\"Тестируем {quant} (ctx {ctx})...\")\n",
        "\n",
        "        ppl, size = run_perplexity_test(model_file, quant, ctx)\n",
        "\n",
        "        if ppl:\n",
        "            results.append({\n",
        "                \"Model\": MODEL_SIZE,\n",
        "                \"Quant\": quant,\n",
        "                \"Context\": ctx,\n",
        "                \"Perplexity\": ppl,\n",
        "                \"Size_GB\": size\n",
        "            })\n",
        "\n",
        "# Сохранение результатов\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(\"results.csv\", index=False)\n",
        "df\n",
        "\n"
      ],
      "metadata": {
        "id": "KdtzLP9CL2XQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "26934604-9423-468d-fb8b-6ad743b2f324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Тестируем q3_k (ctx 256)...\n",
            "5.1690 +/- 0.40510\n",
            "Тестируем q4_k (ctx 256)...\n",
            "5.1165 +/- 0.40442\n",
            "Тестируем q5_k (ctx 256)...\n",
            "5.0548 +/- 0.39835\n",
            "Тестируем q8_0 (ctx 256)...\n",
            "5.0788 +/- 0.40222\n",
            "Тестируем f16 (ctx 256)...\n",
            "5.0728 +/- 0.40156\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Model Quant Context          Perplexity    Size_GB\n",
              "0   13B  q3_k     256  5.1690 +/- 0.40510   5.902540\n",
              "1   13B  q4_k     256  5.1165 +/- 0.40442   7.325775\n",
              "2   13B  q5_k     256  5.0548 +/- 0.39835   8.596069\n",
              "3   13B  q8_0     256  5.0788 +/- 0.40222  12.881452\n",
              "4   13B   f16     256  5.0728 +/- 0.40156  24.245436"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6c0f8003-7699-4a5f-a62a-ce3d452ff388\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Quant</th>\n",
              "      <th>Context</th>\n",
              "      <th>Perplexity</th>\n",
              "      <th>Size_GB</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>13B</td>\n",
              "      <td>q3_k</td>\n",
              "      <td>256</td>\n",
              "      <td>5.1690 +/- 0.40510</td>\n",
              "      <td>5.902540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13B</td>\n",
              "      <td>q4_k</td>\n",
              "      <td>256</td>\n",
              "      <td>5.1165 +/- 0.40442</td>\n",
              "      <td>7.325775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13B</td>\n",
              "      <td>q5_k</td>\n",
              "      <td>256</td>\n",
              "      <td>5.0548 +/- 0.39835</td>\n",
              "      <td>8.596069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13B</td>\n",
              "      <td>q8_0</td>\n",
              "      <td>256</td>\n",
              "      <td>5.0788 +/- 0.40222</td>\n",
              "      <td>12.881452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13B</td>\n",
              "      <td>f16</td>\n",
              "      <td>256</td>\n",
              "      <td>5.0728 +/- 0.40156</td>\n",
              "      <td>24.245436</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c0f8003-7699-4a5f-a62a-ce3d452ff388')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6c0f8003-7699-4a5f-a62a-ce3d452ff388 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6c0f8003-7699-4a5f-a62a-ce3d452ff388');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1edb762f-c796-4471-93e4-46b2153d754d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1edb762f-c796-4471-93e4-46b2153d754d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1edb762f-c796-4471-93e4-46b2153d754d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"13B\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Quant\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"q4_k\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Context\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"256\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Perplexity\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"5.1165 +/- 0.40442\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size_GB\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.435082828321095,\n        \"min\": 5.902539998292923,\n        \"max\": 24.245435506105423,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          7.325774937868118\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}
